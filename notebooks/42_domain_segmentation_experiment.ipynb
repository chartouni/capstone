{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 15: Domain-Specific Models\n",
    "\n",
    "**Hypothesis**: Training separate models per research domain will improve F1 beyond the baseline 62.54%.\n",
    "\n",
    "**Motivation**: Wu et al. (2023) demonstrated that citation patterns vary significantly across research domains. Domain-specific models can capture field-specific citation dynamics better than a universal model.\n",
    "\n",
    "**Method**:\n",
    "1. Group papers by ASJC research field into 5-6 major domains\n",
    "2. Train separate LogisticRegression models per domain\n",
    "3. Compare domain-specific F1 scores vs. baseline (62.54%)\n",
    "4. Calculate overall weighted F1 across all domains\n",
    "\n",
    "**Expected Outcome**: F1 improvement to 63-66% if domain segmentation captures field-specific patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Analyze ASJC Field Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_pickle('../data/processed/cleaned_data.pkl')\n",
    "\n",
    "# Load features and targets\n",
    "X_all = pd.read_pickle('../data/features/X_all.pkl')\n",
    "y_cls = pd.read_pickle('../data/features/y_classification.pkl')\n",
    "metadata = pd.read_pickle('../data/features/metadata.pkl')\n",
    "\n",
    "print(f\"Dataset: {df.shape}\")\n",
    "print(f\"Features: {X_all.shape}\")\n",
    "print(f\"Target: {y_cls.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ASJC field availability\n",
    "print(\"\\n=== ASJC Field Columns ===\")\n",
    "asjc_cols = [col for col in df.columns if 'asjc' in col.lower() or 'field' in col.lower()]\n",
    "print(asjc_cols)\n",
    "\n",
    "if 'ASJC field name' in df.columns:\n",
    "    print(\"\\n=== Top 20 ASJC Fields ===\")\n",
    "    field_dist = df['ASJC field name'].value_counts().head(20)\n",
    "    print(field_dist)\n",
    "    \n",
    "    print(f\"\\nTotal unique fields: {df['ASJC field name'].nunique()}\")\n",
    "    print(f\"Papers with field data: {df['ASJC field name'].notna().sum()} / {len(df)}\")\n",
    "    print(f\"Missing field data: {df['ASJC field name'].isna().sum()}\")\n",
    "else:\n",
    "    print(\"\\nWarning: 'ASJC field name' not found. Available columns:\")\n",
    "    print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Domain Groupings\n",
    "\n",
    "Group ASJC fields into 5-6 major research domains based on common categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define domain mapping (ASJC field ‚Üí broader domain)\n",
    "domain_mapping = {\n",
    "    # Medicine & Health Sciences\n",
    "    'Medicine': 'Medicine & Health',\n",
    "    'Nursing': 'Medicine & Health',\n",
    "    'Health Professions': 'Medicine & Health',\n",
    "    'Dentistry': 'Medicine & Health',\n",
    "    'Pharmacology, Toxicology and Pharmaceutics': 'Medicine & Health',\n",
    "    'Immunology and Microbiology': 'Medicine & Health',\n",
    "    'Neuroscience': 'Medicine & Health',\n",
    "    \n",
    "    # Life & Natural Sciences\n",
    "    'Biochemistry, Genetics and Molecular Biology': 'Life Sciences',\n",
    "    'Agricultural and Biological Sciences': 'Life Sciences',\n",
    "    'Environmental Science': 'Life Sciences',\n",
    "    'Chemistry': 'Natural Sciences',\n",
    "    'Physics and Astronomy': 'Natural Sciences',\n",
    "    'Earth and Planetary Sciences': 'Natural Sciences',\n",
    "    'Mathematics': 'Natural Sciences',\n",
    "    \n",
    "    # Engineering & Technology\n",
    "    'Engineering': 'Engineering & Technology',\n",
    "    'Computer Science': 'Engineering & Technology',\n",
    "    'Materials Science': 'Engineering & Technology',\n",
    "    'Chemical Engineering': 'Engineering & Technology',\n",
    "    'Energy': 'Engineering & Technology',\n",
    "    \n",
    "    # Social Sciences\n",
    "    'Social Sciences': 'Social Sciences',\n",
    "    'Psychology': 'Social Sciences',\n",
    "    'Economics, Econometrics and Finance': 'Social Sciences',\n",
    "    'Business, Management and Accounting': 'Social Sciences',\n",
    "    'Decision Sciences': 'Social Sciences',\n",
    "    \n",
    "    # Arts & Humanities\n",
    "    'Arts and Humanities': 'Arts & Humanities',\n",
    "    \n",
    "    # Multidisciplinary\n",
    "    'Multidisciplinary': 'Multidisciplinary',\n",
    "}\n",
    "\n",
    "# Apply domain mapping\n",
    "if 'ASJC field name' in df.columns:\n",
    "    df['domain'] = df['ASJC field name'].map(domain_mapping)\n",
    "    df['domain'].fillna('Other', inplace=True)\n",
    "    \n",
    "    print(\"=== Domain Distribution ===\")\n",
    "    domain_dist = df['domain'].value_counts()\n",
    "    print(domain_dist)\n",
    "    print(f\"\\nTotal domains: {df['domain'].nunique()}\")\n",
    "else:\n",
    "    print(\"Cannot create domain mapping without ASJC field name column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Train/Test Splits with Domain Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing temporal splits\n",
    "X_train = pd.read_pickle('../data/features/X_train_temporal.pkl')\n",
    "X_test = pd.read_pickle('../data/features/X_test_temporal.pkl')\n",
    "y_train = pd.read_pickle('../data/features/y_train_cls_temporal.pkl')\n",
    "y_test = pd.read_pickle('../data/features/y_test_cls_temporal.pkl')\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Get domain labels for train/test sets\n",
    "if 'domain' in df.columns:\n",
    "    domains_train = df.loc[X_train.index, 'domain']\n",
    "    domains_test = df.loc[X_test.index, 'domain']\n",
    "    \n",
    "    print(\"\\n=== Train Set Domain Distribution ===\")\n",
    "    print(domains_train.value_counts())\n",
    "    \n",
    "    print(\"\\n=== Test Set Domain Distribution ===\")\n",
    "    print(domains_test.value_counts())\n",
    "else:\n",
    "    print(\"\\nError: Domain labels not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model (Universal, No Domain Segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BASELINE: Universal Model (No Domain Segmentation)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train baseline model\n",
    "model_baseline = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "model_baseline.fit(X_train, y_train)\n",
    "y_pred_proba_baseline = model_baseline.predict_proba(X_test)[:, 1]\n",
    "y_pred_baseline = (y_pred_proba_baseline >= 0.54).astype(int)\n",
    "\n",
    "baseline_results = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'Precision': precision_score(y_test, y_pred_baseline),\n",
    "    'Recall': recall_score(y_test, y_pred_baseline),\n",
    "    'F1': f1_score(y_test, y_pred_baseline),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_baseline)\n",
    "}\n",
    "\n",
    "print(\"\\nBaseline Results:\")\n",
    "for metric, value in baseline_results.items():\n",
    "    print(f\"  {metric:12s}: {value*100:.2f}%\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Domain-Specific Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOMAIN-SPECIFIC MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train separate model for each domain\n",
    "domain_models = {}\n",
    "domain_results = {}\n",
    "\n",
    "for domain in sorted(domains_train.unique()):\n",
    "    print(f\"\\n--- {domain} ---\")\n",
    "    \n",
    "    # Get domain-specific train/test data\n",
    "    train_mask = domains_train == domain\n",
    "    test_mask = domains_test == domain\n",
    "    \n",
    "    X_train_domain = X_train[train_mask]\n",
    "    y_train_domain = y_train[train_mask]\n",
    "    X_test_domain = X_test[test_mask]\n",
    "    y_test_domain = y_test[test_mask]\n",
    "    \n",
    "    n_train = len(X_train_domain)\n",
    "    n_test = len(X_test_domain)\n",
    "    \n",
    "    print(f\"Train: {n_train} papers, Test: {n_test} papers\")\n",
    "    \n",
    "    # Skip if too few test samples\n",
    "    if n_test < 50:\n",
    "        print(f\"  ‚ö†Ô∏è  Skipping - too few test samples (<50)\")\n",
    "        continue\n",
    "    \n",
    "    # Check class distribution\n",
    "    if y_train_domain.sum() < 10 or (len(y_train_domain) - y_train_domain.sum()) < 10:\n",
    "        print(f\"  ‚ö†Ô∏è  Skipping - insufficient class balance (need >10 samples per class)\")\n",
    "        continue\n",
    "    \n",
    "    # Train domain-specific model\n",
    "    model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_domain, y_train_domain)\n",
    "    domain_models[domain] = model\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_proba = model.predict_proba(X_test_domain)[:, 1]\n",
    "    y_pred = (y_pred_proba >= 0.54).astype(int)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = {\n",
    "        'n_train': n_train,\n",
    "        'n_test': n_test,\n",
    "        'accuracy': accuracy_score(y_test_domain, y_pred),\n",
    "        'precision': precision_score(y_test_domain, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test_domain, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test_domain, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_test_domain, y_pred_proba) if len(np.unique(y_test_domain)) > 1 else np.nan\n",
    "    }\n",
    "    \n",
    "    domain_results[domain] = results\n",
    "    \n",
    "    print(f\"  F1: {results['f1']*100:.2f}%, ROC-AUC: {results['roc_auc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Overall Domain-Specific Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for all test samples using appropriate domain model\n",
    "y_pred_domain_all = np.zeros(len(y_test))\n",
    "y_pred_proba_domain_all = np.zeros(len(y_test))\n",
    "\n",
    "for domain, model in domain_models.items():\n",
    "    test_mask = domains_test == domain\n",
    "    if test_mask.sum() > 0:\n",
    "        X_test_domain = X_test[test_mask]\n",
    "        y_pred_proba = model.predict_proba(X_test_domain)[:, 1]\n",
    "        y_pred = (y_pred_proba >= 0.54).astype(int)\n",
    "        \n",
    "        y_pred_domain_all[test_mask] = y_pred\n",
    "        y_pred_proba_domain_all[test_mask] = y_pred_proba\n",
    "\n",
    "# Overall metrics\n",
    "overall_results = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_domain_all),\n",
    "    'Precision': precision_score(y_test, y_pred_domain_all),\n",
    "    'Recall': recall_score(y_test, y_pred_domain_all),\n",
    "    'F1': f1_score(y_test, y_pred_domain_all),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_domain_all)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL DOMAIN-SPECIFIC RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nOverall Results (Weighted across all domains):\")\n",
    "for metric, value in overall_results.items():\n",
    "    print(f\"  {metric:12s}: {value*100:.2f}%\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_domain_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Baseline vs Domain-Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: BASELINE vs DOMAIN-SPECIFIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Baseline (Universal)': baseline_results,\n",
    "    'Domain-Specific': overall_results,\n",
    "    'Change': {k: overall_results[k] - baseline_results[k] for k in baseline_results.keys()}\n",
    "})\n",
    "\n",
    "# Format as percentages\n",
    "comparison_df_display = comparison_df.copy()\n",
    "for col in comparison_df_display.columns:\n",
    "    comparison_df_display[col] = comparison_df_display[col].apply(\n",
    "        lambda x: f\"{x*100:+.2f}%\" if isinstance(x, float) else x\n",
    "    )\n",
    "\n",
    "print(\"\\n\", comparison_df_display)\n",
    "\n",
    "# Determine improvement\n",
    "f1_change = overall_results['F1'] - baseline_results['F1']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if f1_change > 0.01:  # More than 1 percentage point\n",
    "    print(f\"‚úÖ IMPROVEMENT: +{f1_change*100:.2f} F1 points\")\n",
    "    print(f\"   Domain-specific models perform better!\")\n",
    "    print(f\"   F1: {baseline_results['F1']*100:.2f}% ‚Üí {overall_results['F1']*100:.2f}%\")\n",
    "elif f1_change > 0:\n",
    "    print(f\"‚ö†Ô∏è  SLIGHT IMPROVEMENT: +{f1_change*100:.2f} F1 points\")\n",
    "    print(f\"   Marginal benefit from domain segmentation\")\n",
    "else:\n",
    "    print(f\"‚ùå NO IMPROVEMENT: {f1_change*100:+.2f} F1 points\")\n",
    "    print(f\"   Domain segmentation did not help with current dataset size\")\n",
    "    print(f\"   Likely due to small sample sizes per domain\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Domain Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed comparison table per domain\n",
    "domain_comparison = []\n",
    "\n",
    "for domain in sorted(domain_results.keys()):\n",
    "    # Get baseline performance for this domain\n",
    "    test_mask = domains_test == domain\n",
    "    y_test_domain = y_test[test_mask]\n",
    "    y_pred_baseline_domain = y_pred_baseline[test_mask]\n",
    "    \n",
    "    baseline_f1_domain = f1_score(y_test_domain, y_pred_baseline_domain, zero_division=0)\n",
    "    domain_f1 = domain_results[domain]['f1']\n",
    "    \n",
    "    domain_comparison.append({\n",
    "        'Domain': domain,\n",
    "        'Test Size': domain_results[domain]['n_test'],\n",
    "        'Baseline F1': baseline_f1_domain,\n",
    "        'Domain-Specific F1': domain_f1,\n",
    "        'Change': domain_f1 - baseline_f1_domain\n",
    "    })\n",
    "\n",
    "domain_comp_df = pd.DataFrame(domain_comparison)\n",
    "domain_comp_df = domain_comp_df.sort_values('Change', ascending=False)\n",
    "\n",
    "print(\"\\n=== Per-Domain F1 Comparison ===\")\n",
    "print(domain_comp_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(domain_comp_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, domain_comp_df['Baseline F1']*100, width, label='Baseline (Universal)', alpha=0.8)\n",
    "ax.bar(x + width/2, domain_comp_df['Domain-Specific F1']*100, width, label='Domain-Specific', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Research Domain')\n",
    "ax.set_ylabel('F1 Score (%)')\n",
    "ax.set_title('F1 Score Comparison: Baseline vs Domain-Specific Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(domain_comp_df['Domain'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.axhline(y=baseline_results['F1']*100, color='red', linestyle='--', \n",
    "           label=f\"Overall Baseline: {baseline_results['F1']*100:.2f}%\", alpha=0.5)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion & Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüí° CONCLUSION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if f1_change > 0.01:\n",
    "    print(\"‚úÖ Domain-specific modeling IMPROVED performance!\")\n",
    "    print(f\"\\n   Overall F1 increased by {f1_change*100:.2f} points\")\n",
    "    print(f\"   This validates Wu et al.'s (2023) findings on domain segmentation.\")\n",
    "    print(f\"\\n   Recommendation: With a larger dataset (20,000+ papers), domain-specific\")\n",
    "    print(f\"   models could achieve F1 of 65-70%.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Domain-specific modeling did NOT improve performance.\")\n",
    "    print(f\"\\n   Change: {f1_change*100:+.2f} F1 points (not significant)\")\n",
    "    print(f\"\\n   Likely reasons:\")\n",
    "    print(f\"   1. Dataset size: Training sets per domain are small (200-800 papers)\")\n",
    "    print(f\"   2. Wu et al. used 4M+ papers, enabling robust domain-specific models\")\n",
    "    print(f\"   3. Current universal model already captures most patterns\")\n",
    "    print(f\"\\n   Recommendation: Domain segmentation requires larger dataset.\")\n",
    "    print(f\"   Baseline (62.54% F1) remains optimal for current data size.\")\n",
    "\n",
    "print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
