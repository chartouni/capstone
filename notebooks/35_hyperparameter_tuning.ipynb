{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Classification and Regression\n",
    "\n",
    "Optimize model hyperparameters to improve performance on 2010-2021 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, f1_score, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load temporal split\n",
    "X_train = pd.read_pickle('../data/features/X_train_temporal.pkl')\n",
    "X_test = pd.read_pickle('../data/features/X_test_temporal.pkl')\n",
    "y_train_cls = pd.read_pickle('../data/features/y_train_cls_temporal.pkl')\n",
    "y_test_cls = pd.read_pickle('../data/features/y_test_cls_temporal.pkl')\n",
    "y_train_reg = pd.read_pickle('../data/features/y_train_reg_temporal.pkl')\n",
    "y_test_reg = pd.read_pickle('../data/features/y_test_reg_temporal.pkl')\n",
    "\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification Hyperparameter Tuning\n",
    "\n",
    "### LightGBM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning LightGBM Classifier...\")\n",
    "\n",
    "# Hyperparameter search space\n",
    "lgb_params = {\n",
    "    'num_leaves': [20, 31, 40, 50],\n",
    "    'max_depth': [5, 7, 10, 15, -1],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'min_child_samples': [10, 20, 30, 50],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "\n",
    "# Randomized search (faster than grid search)\n",
    "lgb_search = RandomizedSearchCV(\n",
    "    lgb_clf,\n",
    "    lgb_params,\n",
    "    n_iter=50,  # Try 50 random combinations\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_search.fit(X_train, y_train_cls)\n",
    "\n",
    "print(f\"\\nBest CV ROC-AUC: {lgb_search.best_score_:.4f}\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in lgb_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance\n",
    "y_pred_lgb = lgb_search.best_estimator_.predict(X_test)\n",
    "y_pred_proba_lgb = lgb_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "lgb_roc_auc = roc_auc_score(y_test_cls, y_pred_proba_lgb)\n",
    "lgb_f1 = f1_score(y_test_cls, y_pred_lgb)\n",
    "\n",
    "print(f\"\\nLightGBM Test Performance:\")\n",
    "print(f\"  ROC-AUC: {lgb_roc_auc:.4f}\")\n",
    "print(f\"  F1-Score: {lgb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning XGBoost Classifier...\")\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_clf,\n",
    "    xgb_params,\n",
    "    n_iter=50,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train_cls)\n",
    "\n",
    "print(f\"\\nBest CV ROC-AUC: {xgb_search.best_score_:.4f}\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in xgb_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance\n",
    "y_pred_xgb = xgb_search.best_estimator_.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "xgb_roc_auc = roc_auc_score(y_test_cls, y_pred_proba_xgb)\n",
    "xgb_f1 = f1_score(y_test_cls, y_pred_xgb)\n",
    "\n",
    "print(f\"\\nXGBoost Test Performance:\")\n",
    "print(f\"  ROC-AUC: {xgb_roc_auc:.4f}\")\n",
    "print(f\"  F1-Score: {xgb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression Hyperparameter Tuning\n",
    "\n",
    "### LightGBM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning LightGBM Regressor...\")\n",
    "\n",
    "lgb_reg_params = {\n",
    "    'num_leaves': [20, 31, 40, 50],\n",
    "    'max_depth': [5, 7, 10, 15, -1],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'min_child_samples': [10, 20, 30, 50],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "lgb_reg = lgb.LGBMRegressor(random_state=42, verbose=-1)\n",
    "\n",
    "lgb_reg_search = RandomizedSearchCV(\n",
    "    lgb_reg,\n",
    "    lgb_reg_params,\n",
    "    n_iter=50,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_reg_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(f\"\\nBest CV R²: {lgb_reg_search.best_score_:.4f}\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in lgb_reg_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance\n",
    "y_pred_lgb_reg = lgb_reg_search.best_estimator_.predict(X_test)\n",
    "\n",
    "lgb_r2 = r2_score(y_test_reg, y_pred_lgb_reg)\n",
    "lgb_spearman = spearmanr(y_test_reg, y_pred_lgb_reg)[0]\n",
    "\n",
    "print(f\"\\nLightGBM Regression Test Performance:\")\n",
    "print(f\"  R²: {lgb_r2:.4f}\")\n",
    "print(f\"  Spearman: {lgb_spearman:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning XGBoost Regressor...\")\n",
    "\n",
    "xgb_reg_params = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "xgb_reg_search = RandomizedSearchCV(\n",
    "    xgb_reg,\n",
    "    xgb_reg_params,\n",
    "    n_iter=50,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_reg_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(f\"\\nBest CV R²: {xgb_reg_search.best_score_:.4f}\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in xgb_reg_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance\n",
    "y_pred_xgb_reg = xgb_reg_search.best_estimator_.predict(X_test)\n",
    "\n",
    "xgb_r2 = r2_score(y_test_reg, y_pred_xgb_reg)\n",
    "xgb_spearman = spearmanr(y_test_reg, y_pred_xgb_reg)[0]\n",
    "\n",
    "print(f\"\\nXGBoost Regression Test Performance:\")\n",
    "print(f\"  R²: {xgb_r2:.4f}\")\n",
    "print(f\"  Spearman: {xgb_spearman:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save classification models\n",
    "with open(models_dir / 'lgbm_classifier_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(lgb_search.best_estimator_, f)\n",
    "\n",
    "with open(models_dir / 'xgb_classifier_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_search.best_estimator_, f)\n",
    "\n",
    "# Save regression models\n",
    "with open(models_dir / 'lgbm_regressor_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(lgb_reg_search.best_estimator_, f)\n",
    "\n",
    "with open(models_dir / 'xgb_regressor_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_reg_search.best_estimator_, f)\n",
    "\n",
    "print(\"✓ All tuned models saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nCLASSIFICATION (Test Set):\")\n",
    "print(f\"  LightGBM - ROC-AUC: {lgb_roc_auc:.4f}, F1: {lgb_f1:.4f}\")\n",
    "print(f\"  XGBoost  - ROC-AUC: {xgb_roc_auc:.4f}, F1: {xgb_f1:.4f}\")\n",
    "\n",
    "print(\"\\nREGRESSION (Test Set):\")\n",
    "print(f\"  LightGBM - R²: {lgb_r2:.4f}, Spearman: {lgb_spearman:.4f}\")\n",
    "print(f\"  XGBoost  - R²: {xgb_r2:.4f}, Spearman: {xgb_spearman:.4f}\")\n",
    "\n",
    "# Compare to baseline (before tuning)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON TO BASELINE (Before Tuning)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nClassification:\")\n",
    "print(f\"  Baseline: ROC-AUC=78.74%, F1=56.55%\")\n",
    "print(f\"  Best tuned: ROC-AUC={max(lgb_roc_auc, xgb_roc_auc)*100:.2f}%, F1={max(lgb_f1, xgb_f1)*100:.2f}%\")\n",
    "print(f\"  Improvement: {(max(lgb_roc_auc, xgb_roc_auc) - 0.7874)*100:.2f} points ROC-AUC, {(max(lgb_f1, xgb_f1) - 0.5655)*100:.2f} points F1\")\n",
    "\n",
    "print(\"\\nRegression:\")\n",
    "print(f\"  Baseline: R²=24.25%, Spearman=56.70%\")\n",
    "print(f\"  Best tuned: R²={max(lgb_r2, xgb_r2)*100:.2f}%, Spearman={max(lgb_spearman, xgb_spearman)*100:.2f}%\")\n",
    "print(f\"  Improvement: {(max(lgb_r2, xgb_r2) - 0.2425)*100:.2f} points R², {(max(lgb_spearman, xgb_spearman) - 0.5670)*100:.2f} points Spearman\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
