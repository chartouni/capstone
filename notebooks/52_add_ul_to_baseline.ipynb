{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add UL Features to Existing Baseline\n",
    "\n",
    "**Goal**: Simply add UL features to your existing 63% F1 baseline model\n",
    "\n",
    "**Approach**:\n",
    "1. Load your existing features (from 24_rebuild_temporal_split.ipynb)\n",
    "2. Load UL features (from 50_unsupervised_learning.ipynb)\n",
    "3. Merge them together (excluding leakage features)\n",
    "4. Save enhanced feature sets\n",
    "5. Re-run your existing classification notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Existing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your existing temporal split features\n",
    "feature_dir = Path('../data/features')\n",
    "\n",
    "# Check if files exist\n",
    "if not feature_dir.exists():\n",
    "    print(\"⚠️  Feature directory doesn't exist!\")\n",
    "    print(\"Run notebooks/24_rebuild_temporal_split.ipynb first to generate features.\")\n",
    "else:\n",
    "    print(f\"✓ Feature directory found: {feature_dir}\")\n",
    "    print(f\"\\nAvailable files:\")\n",
    "    for f in sorted(feature_dir.glob('*.pkl')):\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing features\n",
    "X_train = pd.read_pickle(feature_dir / 'X_train_temporal.pkl')\n",
    "X_test = pd.read_pickle(feature_dir / 'X_test_temporal.pkl')\n",
    "y_train_cls = pd.read_pickle(feature_dir / 'y_train_cls_temporal.pkl')\n",
    "y_test_cls = pd.read_pickle(feature_dir / 'y_test_cls_temporal.pkl')\n",
    "\n",
    "print(f\"Baseline Features:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  y_train: {y_train_cls.shape} (high-impact: {y_train_cls.mean()*100:.1f}%)\")\n",
    "print(f\"  y_test: {y_test_cls.shape} (high-impact: {y_test_cls.mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nBaseline feature names ({len(X_train.columns)}):\")\n",
    "for i, col in enumerate(X_train.columns, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load UL Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with unsupervised features\n",
    "df_ul = pd.read_pickle('../data/processed/data_with_unsupervised_features.pkl')\n",
    "\n",
    "print(f\"Data with UL features: {df_ul.shape}\")\n",
    "print(f\"Date range: {df_ul['Year'].min()}-{df_ul['Year'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify UL features (EXCLUDE leakage features!)\n",
    "ul_topic_features = [c for c in df_ul.columns if c.startswith('topic_')]\n",
    "ul_nmf_features = [c for c in df_ul.columns if c.startswith('nmf_topic_')]\n",
    "ul_pca_features = [c for c in df_ul.columns if c.startswith('pca_')]\n",
    "ul_cluster_features = [c for c in df_ul.columns if 'cluster' in c.lower() and 'venue' not in c.lower()]\n",
    "ul_other_features = ['dominant_topic', 'dominant_topic_weight', 'nmf_dominant_topic']\n",
    "\n",
    "# CRITICAL: Exclude data leakage features!\n",
    "leakage_features = ['citation_zscore', 'is_citation_outlier']\n",
    "\n",
    "all_ul_features = (\n",
    "    ul_topic_features + \n",
    "    ul_nmf_features + \n",
    "    ul_pca_features + \n",
    "    ul_cluster_features + \n",
    "    [f for f in ul_other_features if f in df_ul.columns]\n",
    ")\n",
    "\n",
    "# Remove leakage features\n",
    "all_ul_features = [f for f in all_ul_features if f not in leakage_features]\n",
    "\n",
    "print(f\"Unsupervised Learning Features (Leakage-Free):\")\n",
    "print(f\"  LDA topics: {len(ul_topic_features)}\")\n",
    "print(f\"  NMF topics: {len(ul_nmf_features)}\")\n",
    "print(f\"  PCA components: {len(ul_pca_features)}\")\n",
    "print(f\"  Cluster labels: {len(ul_cluster_features)}\")\n",
    "print(f\"  Other: {len([f for f in ul_other_features if f in df_ul.columns])}\")\n",
    "print(f\"  Total: {len(all_ul_features)}\")\n",
    "print(f\"\\n⚠️  EXCLUDED (data leakage): {leakage_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Align and Merge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices from existing splits\n",
    "train_indices = X_train.index\n",
    "test_indices = X_test.index\n",
    "\n",
    "print(f\"Train indices: {len(train_indices)}\")\n",
    "print(f\"Test indices: {len(test_indices)}\")\n",
    "\n",
    "# Extract UL features for train/test splits\n",
    "ul_train = df_ul.loc[train_indices, all_ul_features].copy()\n",
    "ul_test = df_ul.loc[test_indices, all_ul_features].copy()\n",
    "\n",
    "print(f\"\\nUL features extracted:\")\n",
    "print(f\"  ul_train: {ul_train.shape}\")\n",
    "print(f\"  ul_test: {ul_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge baseline + UL features\n",
    "X_train_enhanced = pd.concat([X_train, ul_train], axis=1)\n",
    "X_test_enhanced = pd.concat([X_test, ul_test], axis=1)\n",
    "\n",
    "print(f\"Enhanced Feature Sets:\")\n",
    "print(f\"  X_train_enhanced: {X_train_enhanced.shape}\")\n",
    "print(f\"  X_test_enhanced: {X_test_enhanced.shape}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  Baseline features: {X_train.shape[1]}\")\n",
    "print(f\"  UL features added: {ul_train.shape[1]}\")\n",
    "print(f\"  Total features: {X_train_enhanced.shape[1]}\")\n",
    "\n",
    "# Check for any NaN values\n",
    "print(f\"\\nMissing values:\")\n",
    "print(f\"  X_train_enhanced: {X_train_enhanced.isna().sum().sum()}\")\n",
    "print(f\"  X_test_enhanced: {X_test_enhanced.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handle Missing Values (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with median (if any)\n",
    "if X_train_enhanced.isna().sum().sum() > 0:\n",
    "    print(\"Filling missing values with median...\")\n",
    "    \n",
    "    for col in X_train_enhanced.columns:\n",
    "        if X_train_enhanced[col].isna().any():\n",
    "            median_val = X_train_enhanced[col].median()\n",
    "            X_train_enhanced[col].fillna(median_val, inplace=True)\n",
    "            X_test_enhanced[col].fillna(median_val, inplace=True)\n",
    "            print(f\"  Filled {col} with {median_val}\")\n",
    "    \n",
    "    print(f\"\\n✓ All missing values filled\")\n",
    "else:\n",
    "    print(\"✓ No missing values - data is clean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Enhanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced feature sets\n",
    "X_train_enhanced.to_pickle(feature_dir / 'X_train_temporal_with_ul.pkl')\n",
    "X_test_enhanced.to_pickle(feature_dir / 'X_test_temporal_with_ul.pkl')\n",
    "\n",
    "# Also save the UL feature list for reference\n",
    "with open(feature_dir / 'ul_features_list.txt', 'w') as f:\n",
    "    f.write(f\"Unsupervised Learning Features ({len(all_ul_features)})\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    for feat in all_ul_features:\n",
    "        f.write(f\"{feat}\\n\")\n",
    "\n",
    "print(\"✓ Enhanced features saved:\")\n",
    "print(f\"  {feature_dir / 'X_train_temporal_with_ul.pkl'}\")\n",
    "print(f\"  {feature_dir / 'X_test_temporal_with_ul.pkl'}\")\n",
    "print(f\"  {feature_dir / 'ul_features_list.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Comparison Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "print(\"Quick comparison: Baseline vs Enhanced...\\n\")\n",
    "\n",
    "# Test baseline\n",
    "print(\"Training baseline model...\")\n",
    "clf_baseline = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "clf_baseline.fit(X_train, y_train_cls)\n",
    "y_pred_baseline = clf_baseline.predict(X_test)\n",
    "y_proba_baseline = clf_baseline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "baseline_f1 = f1_score(y_test_cls, y_pred_baseline)\n",
    "baseline_auc = roc_auc_score(y_test_cls, y_proba_baseline)\n",
    "baseline_precision = precision_score(y_test_cls, y_pred_baseline)\n",
    "baseline_recall = recall_score(y_test_cls, y_pred_baseline)\n",
    "\n",
    "print(f\"Baseline Results:\")\n",
    "print(f\"  F1: {baseline_f1:.4f}\")\n",
    "print(f\"  Precision: {baseline_precision:.4f}\")\n",
    "print(f\"  Recall: {baseline_recall:.4f}\")\n",
    "print(f\"  ROC-AUC: {baseline_auc:.4f}\")\n",
    "\n",
    "# Test enhanced\n",
    "print(f\"\\nTraining enhanced model (+UL features)...\")\n",
    "clf_enhanced = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "clf_enhanced.fit(X_train_enhanced, y_train_cls)\n",
    "y_pred_enhanced = clf_enhanced.predict(X_test_enhanced)\n",
    "y_proba_enhanced = clf_enhanced.predict_proba(X_test_enhanced)[:, 1]\n",
    "\n",
    "enhanced_f1 = f1_score(y_test_cls, y_pred_enhanced)\n",
    "enhanced_auc = roc_auc_score(y_test_cls, y_proba_enhanced)\n",
    "enhanced_precision = precision_score(y_test_cls, y_pred_enhanced)\n",
    "enhanced_recall = recall_score(y_test_cls, y_pred_enhanced)\n",
    "\n",
    "print(f\"Enhanced Results:\")\n",
    "print(f\"  F1: {enhanced_f1:.4f} ({enhanced_f1-baseline_f1:+.4f})\")\n",
    "print(f\"  Precision: {enhanced_precision:.4f} ({enhanced_precision-baseline_precision:+.4f})\")\n",
    "print(f\"  Recall: {enhanced_recall:.4f} ({enhanced_recall-baseline_recall:+.4f})\")\n",
    "print(f\"  ROC-AUC: {enhanced_auc:.4f} ({enhanced_auc-baseline_auc:+.4f})\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPROVEMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "f1_improvement = ((enhanced_f1 - baseline_f1) / baseline_f1 * 100)\n",
    "auc_improvement = ((enhanced_auc - baseline_auc) / baseline_auc * 100)\n",
    "\n",
    "print(f\"F1 Score: {baseline_f1:.4f} → {enhanced_f1:.4f} ({f1_improvement:+.2f}%)\")\n",
    "print(f\"ROC-AUC: {baseline_auc:.4f} → {enhanced_auc:.4f} ({auc_improvement:+.2f}%)\")\n",
    "\n",
    "if enhanced_f1 > baseline_f1:\n",
    "    print(f\"\\n✅ UL features improved F1 by {f1_improvement:.2f}%\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  UL features decreased F1 by {abs(f1_improvement):.2f}% (might need feature selection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "Now you can:\n",
    "\n",
    "1. **Run your existing classification notebook** (30_classification_models.ipynb):\n",
    "   - Just load `X_train_temporal_with_ul.pkl` instead of `X_train_temporal.pkl`\n",
    "   - Compare all your models with enhanced features\n",
    "\n",
    "2. **Feature selection** (if needed):\n",
    "   - Select only top N UL features by correlation\n",
    "   - Use feature importance from Random Forest\n",
    "   - Try different UL feature combinations\n",
    "\n",
    "3. **Hyperparameter tuning**:\n",
    "   - Re-tune your models with the enhanced feature set\n",
    "   - May need different max_depth, n_estimators, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
