{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Unsupervised Learning Feature Integration\n\n**Goal**: Integrate unsupervised learning features into supervised models and measure performance improvement\n\n**⚠️ CRITICAL - Data Leakage Prevention**:\n- We EXCLUDE `citation_zscore` and `is_citation_outlier` features\n- These are derived from the target variable (Citations) and would cause data leakage\n- Using them would give artificially high performance that won't generalize\n\n**Approach**:\n1. Load data with UL features\n2. Remove leakage features (citation_zscore, is_citation_outlier)\n3. Select top-performing UL features based on correlation\n4. Train baseline models (without UL features)\n5. Train enhanced models (with UL features)\n6. Compare performance and analyze improvements\n7. Feature importance analysis\n8. Recommendations for production models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with unsupervised features\n",
    "df = pd.read_pickle('../data/processed/data_with_unsupervised_features.pkl')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"\\nDate range: {df['Year'].min()} - {df['Year'].max()}\")\n",
    "print(f\"Citation stats: mean={df['Citations'].mean():.1f}, median={df['Citations'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Identify feature categories\nul_topic_features = [c for c in df.columns if c.startswith('topic_')]\nul_nmf_features = [c for c in df.columns if c.startswith('nmf_topic_')]\nul_pca_features = [c for c in df.columns if c.startswith('pca_')]\nul_cluster_features = [c for c in df.columns if 'cluster' in c and c != 'venue_cluster']\nul_other_features = ['dominant_topic', 'dominant_topic_weight', 'nmf_dominant_topic']\n\n# CRITICAL: Exclude citation_zscore and is_citation_outlier - they cause DATA LEAKAGE!\n# These features are derived from the target variable (Citations) and would\n# give the model unfair access to the answer during training.\nleakage_features = ['citation_zscore', 'is_citation_outlier']\nprint(f\"⚠️  EXCLUDING LEAKAGE FEATURES: {leakage_features}\")\n\nall_ul_features = (ul_topic_features + ul_nmf_features + ul_pca_features + \n                   ul_cluster_features + [f for f in ul_other_features if f in df.columns])\n\n# Double-check: remove any leakage features that might have slipped in\nall_ul_features = [f for f in all_ul_features if f not in leakage_features]\n\nprint(f\"\\nUnsupervised Learning Features (Leakage-Free):\")\nprint(f\"  LDA topics: {len(ul_topic_features)}\")\nprint(f\"  NMF topics: {len(ul_nmf_features)}\")\nprint(f\"  PCA components: {len(ul_pca_features)}\")\nprint(f\"  Cluster labels: {len(ul_cluster_features)}\")\nprint(f\"  Other: {len([f for f in ul_other_features if f in df.columns])}\")\nprint(f\"  Total UL features: {len(all_ul_features)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection - Top UL Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with Citations\n",
    "ul_correlations = df[all_ul_features + ['Citations']].corr()['Citations'].drop('Citations')\n",
    "ul_correlations_abs = ul_correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 30 UL Features by Correlation with Citations:\")\n",
    "print(ul_correlations_abs.head(30))\n",
    "\n",
    "# Select top features\n",
    "top_n = 20\n",
    "top_ul_features = ul_correlations_abs.head(top_n).index.tolist()\n",
    "\n",
    "print(f\"\\nSelected top {top_n} UL features for modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "ul_correlations_abs.head(30).plot(kind='barh')\n",
    "plt.xlabel('Absolute Correlation with Citations')\n",
    "plt.title('Top 30 Unsupervised Features by Correlation')\n",
    "plt.axvline(x=0.05, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Baseline Features\n",
    "\n",
    "Identify existing engineered features (non-UL) for baseline comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify baseline features (excluding UL features and target)\n",
    "exclude_cols = (\n",
    "    all_ul_features + \n",
    "    ['Citations', 'high_impact', 'log_citations', 'citation_bin', 'citation_category'] +  # targets\n",
    "    ['Title', 'Abstract', 'Authors', 'Author full names', 'Author(s) ID', 'Source title'] +  # text/metadata\n",
    "    ['EID', 'DOI', 'PubMed ID', 'Link', 'Cited by', 'Abstract', 'Document Type']  # identifiers\n",
    ")\n",
    "\n",
    "# Get numeric columns for modeling\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "baseline_features = [c for c in numeric_cols if c not in exclude_cols]\n",
    "\n",
    "print(f\"Available baseline features: {len(baseline_features)}\")\n",
    "print(f\"\\nBaseline features:\")\n",
    "for i, feat in enumerate(baseline_features, 1):\n",
    "    print(f\"  {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification target (high impact = top 25% citations)\n",
    "citation_threshold = df['Citations'].quantile(0.75)\n",
    "df['high_impact'] = (df['Citations'] >= citation_threshold).astype(int)\n",
    "\n",
    "# Create log-transformed regression target\n",
    "df['log_citations'] = np.log1p(df['Citations'])\n",
    "\n",
    "print(f\"Classification target: high_impact (>= {citation_threshold:.0f} citations)\")\n",
    "print(f\"  High impact: {df['high_impact'].sum()} papers ({df['high_impact'].mean()*100:.1f}%)\")\n",
    "print(f\"  Regular: {(~df['high_impact'].astype(bool)).sum()} papers ({(1-df['high_impact'].mean())*100:.1f}%)\")\n",
    "print(f\"\\nRegression target: log(Citations + 1)\")\n",
    "print(f\"  Range: {df['log_citations'].min():.2f} - {df['log_citations'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split - Temporal Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use temporal split: train on earlier years, test on recent years\n",
    "split_year = 2022\n",
    "\n",
    "train_mask = df['Year'] < split_year\n",
    "test_mask = df['Year'] >= split_year\n",
    "\n",
    "df_train = df[train_mask].copy()\n",
    "df_test = df[test_mask].copy()\n",
    "\n",
    "print(f\"Temporal Split (cutoff year: {split_year}):\")\n",
    "print(f\"  Training set: {len(df_train)} papers ({df_train['Year'].min()}-{df_train['Year'].max()})\")\n",
    "print(f\"  Test set: {len(df_test)} papers ({df_test['Year'].min()}-{df_test['Year'].max()})\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Train high_impact: {df_train['high_impact'].mean()*100:.1f}%\")\n",
    "print(f\"  Test high_impact: {df_test['high_impact'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Models (No UL Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data - handle missing values\n",
    "def prepare_features(df, features):\n",
    "    \"\"\"Prepare feature matrix, handling missing values\"\"\"\n",
    "    X = df[features].copy()\n",
    "    # Fill NaN with median for numeric columns\n",
    "    for col in X.columns:\n",
    "        if X[col].isna().any():\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "    return X\n",
    "\n",
    "# Baseline features\n",
    "X_train_baseline = prepare_features(df_train, baseline_features)\n",
    "X_test_baseline = prepare_features(df_test, baseline_features)\n",
    "\n",
    "y_train_class = df_train['high_impact']\n",
    "y_test_class = df_test['high_impact']\n",
    "\n",
    "y_train_reg = df_train['log_citations']\n",
    "y_test_reg = df_test['log_citations']\n",
    "\n",
    "print(f\"Baseline feature matrix: {X_train_baseline.shape}\")\n",
    "print(f\"Features: {baseline_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline classification model\n",
    "print(\"Training Baseline Classification Model (Random Forest)...\")\n",
    "\n",
    "baseline_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "baseline_clf.fit(X_train_baseline, y_train_class)\n",
    "\n",
    "# Predictions\n",
    "y_pred_baseline = baseline_clf.predict(X_test_baseline)\n",
    "y_pred_proba_baseline = baseline_clf.predict_proba(X_test_baseline)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "baseline_f1 = f1_score(y_test_class, y_pred_baseline)\n",
    "baseline_precision = precision_score(y_test_class, y_pred_baseline)\n",
    "baseline_recall = recall_score(y_test_class, y_pred_baseline)\n",
    "baseline_auc = roc_auc_score(y_test_class, y_pred_proba_baseline)\n",
    "\n",
    "print(\"\\nBaseline Classification Results:\")\n",
    "print(f\"  F1 Score: {baseline_f1:.4f}\")\n",
    "print(f\"  Precision: {baseline_precision:.4f}\")\n",
    "print(f\"  Recall: {baseline_recall:.4f}\")\n",
    "print(f\"  ROC-AUC: {baseline_auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline regression model\n",
    "print(\"Training Baseline Regression Model (Random Forest)...\")\n",
    "\n",
    "baseline_reg = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "baseline_reg.fit(X_train_baseline, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_reg_baseline = baseline_reg.predict(X_test_baseline)\n",
    "\n",
    "# Metrics\n",
    "baseline_mae = mean_absolute_error(y_test_reg, y_pred_reg_baseline)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg_baseline))\n",
    "baseline_r2 = r2_score(y_test_reg, y_pred_reg_baseline)\n",
    "\n",
    "print(\"\\nBaseline Regression Results:\")\n",
    "print(f\"  MAE: {baseline_mae:.4f}\")\n",
    "print(f\"  RMSE: {baseline_rmse:.4f}\")\n",
    "print(f\"  R²: {baseline_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Enhanced Models (With Top UL Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced features = baseline + top UL features\n",
    "enhanced_features = baseline_features + top_ul_features\n",
    "\n",
    "X_train_enhanced = prepare_features(df_train, enhanced_features)\n",
    "X_test_enhanced = prepare_features(df_test, enhanced_features)\n",
    "\n",
    "print(f\"Enhanced feature matrix: {X_train_enhanced.shape}\")\n",
    "print(f\"Added {len(top_ul_features)} UL features to {len(baseline_features)} baseline features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train enhanced classification model\n",
    "print(\"Training Enhanced Classification Model (Random Forest + UL)...\")\n",
    "\n",
    "enhanced_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "enhanced_clf.fit(X_train_enhanced, y_train_class)\n",
    "\n",
    "# Predictions\n",
    "y_pred_enhanced = enhanced_clf.predict(X_test_enhanced)\n",
    "y_pred_proba_enhanced = enhanced_clf.predict_proba(X_test_enhanced)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "enhanced_f1 = f1_score(y_test_class, y_pred_enhanced)\n",
    "enhanced_precision = precision_score(y_test_class, y_pred_enhanced)\n",
    "enhanced_recall = recall_score(y_test_class, y_pred_enhanced)\n",
    "enhanced_auc = roc_auc_score(y_test_class, y_pred_proba_enhanced)\n",
    "\n",
    "print(\"\\nEnhanced Classification Results:\")\n",
    "print(f\"  F1 Score: {enhanced_f1:.4f} (Δ={enhanced_f1-baseline_f1:+.4f})\")\n",
    "print(f\"  Precision: {enhanced_precision:.4f} (Δ={enhanced_precision-baseline_precision:+.4f})\")\n",
    "print(f\"  Recall: {enhanced_recall:.4f} (Δ={enhanced_recall-baseline_recall:+.4f})\")\n",
    "print(f\"  ROC-AUC: {enhanced_auc:.4f} (Δ={enhanced_auc-baseline_auc:+.4f})\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_enhanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train enhanced regression model\n",
    "print(\"Training Enhanced Regression Model (Random Forest + UL)...\")\n",
    "\n",
    "enhanced_reg = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "enhanced_reg.fit(X_train_enhanced, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_reg_enhanced = enhanced_reg.predict(X_test_enhanced)\n",
    "\n",
    "# Metrics\n",
    "enhanced_mae = mean_absolute_error(y_test_reg, y_pred_reg_enhanced)\n",
    "enhanced_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg_enhanced))\n",
    "enhanced_r2 = r2_score(y_test_reg, y_pred_reg_enhanced)\n",
    "\n",
    "print(\"\\nEnhanced Regression Results:\")\n",
    "print(f\"  MAE: {enhanced_mae:.4f} (Δ={enhanced_mae-baseline_mae:+.4f})\")\n",
    "print(f\"  RMSE: {enhanced_rmse:.4f} (Δ={enhanced_rmse-baseline_rmse:+.4f})\")\n",
    "print(f\"  R²: {enhanced_r2:.4f} (Δ={enhanced_r2-baseline_r2:+.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall', 'ROC-AUC', 'MAE', 'RMSE', 'R²'],\n",
    "    'Baseline': [baseline_f1, baseline_precision, baseline_recall, baseline_auc, \n",
    "                 baseline_mae, baseline_rmse, baseline_r2],\n",
    "    'Enhanced': [enhanced_f1, enhanced_precision, enhanced_recall, enhanced_auc,\n",
    "                 enhanced_mae, enhanced_rmse, enhanced_r2]\n",
    "})\n",
    "\n",
    "comparison_df['Improvement'] = comparison_df['Enhanced'] - comparison_df['Baseline']\n",
    "comparison_df['Improvement %'] = (comparison_df['Improvement'] / comparison_df['Baseline'] * 100).round(2)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON: Baseline vs Enhanced (with UL features)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Classification metrics\n",
    "class_metrics = ['F1 Score', 'Precision', 'Recall', 'ROC-AUC']\n",
    "class_data = comparison_df[comparison_df['Metric'].isin(class_metrics)]\n",
    "\n",
    "x = np.arange(len(class_metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, class_data['Baseline'], width, label='Baseline', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, class_data['Enhanced'], width, label='Enhanced', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Metric')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('Classification Metrics Comparison')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(class_metrics, rotation=45)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Improvement percentages\n",
    "class_data[['Metric', 'Improvement %']].set_index('Metric')['Improvement %'].plot(\n",
    "    kind='barh', ax=axes[0, 1], color='green', alpha=0.7\n",
    ")\n",
    "axes[0, 1].set_xlabel('Improvement (%)')\n",
    "axes[0, 1].set_title('Classification: % Improvement with UL Features')\n",
    "axes[0, 1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Regression metrics\n",
    "reg_metrics = ['MAE', 'RMSE', 'R²']\n",
    "reg_data = comparison_df[comparison_df['Metric'].isin(reg_metrics)]\n",
    "\n",
    "x = np.arange(len(reg_metrics))\n",
    "\n",
    "axes[1, 0].bar(x - width/2, reg_data['Baseline'], width, label='Baseline', alpha=0.8)\n",
    "axes[1, 0].bar(x + width/2, reg_data['Enhanced'], width, label='Enhanced', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Metric')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_title('Regression Metrics Comparison')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(reg_metrics)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Regression improvement\n",
    "reg_data[['Metric', 'Improvement %']].set_index('Metric')['Improvement %'].plot(\n",
    "    kind='barh', ax=axes[1, 1], color='orange', alpha=0.7\n",
    ")\n",
    "axes[1, 1].set_xlabel('Improvement (%)')\n",
    "axes[1, 1].set_title('Regression: % Improvement with UL Features')\n",
    "axes[1, 1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from enhanced models\n",
    "feature_importance_clf = pd.DataFrame({\n",
    "    'feature': enhanced_features,\n",
    "    'importance': enhanced_clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importance_reg = pd.DataFrame({\n",
    "    'feature': enhanced_features,\n",
    "    'importance': enhanced_reg.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Mark UL features\n",
    "feature_importance_clf['is_ul'] = feature_importance_clf['feature'].isin(top_ul_features)\n",
    "feature_importance_reg['is_ul'] = feature_importance_reg['feature'].isin(top_ul_features)\n",
    "\n",
    "print(\"Top 20 Most Important Features (Classification):\")\n",
    "print(feature_importance_clf.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features (Regression):\")\n",
    "print(feature_importance_reg.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Classification\n",
    "top_20_clf = feature_importance_clf.head(20)\n",
    "colors_clf = ['skyblue' if is_ul else 'lightgray' for is_ul in top_20_clf['is_ul']]\n",
    "\n",
    "axes[0].barh(range(20), top_20_clf['importance'], color=colors_clf)\n",
    "axes[0].set_yticks(range(20))\n",
    "axes[0].set_yticklabels(top_20_clf['feature'])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Top 20 Features: Classification\\n(Blue = UL features)')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Regression\n",
    "top_20_reg = feature_importance_reg.head(20)\n",
    "colors_reg = ['orange' if is_ul else 'lightgray' for is_ul in top_20_reg['is_ul']]\n",
    "\n",
    "axes[1].barh(range(20), top_20_reg['importance'], color=colors_reg)\n",
    "axes[1].set_yticks(range(20))\n",
    "axes[1].set_yticklabels(top_20_reg['feature'])\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Top 20 Features: Regression\\n(Orange = UL features)')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze UL feature contribution\n",
    "ul_importance_clf = feature_importance_clf[feature_importance_clf['is_ul']]\n",
    "ul_importance_reg = feature_importance_reg[feature_importance_reg['is_ul']]\n",
    "\n",
    "print(\"UL Feature Importance Summary:\")\n",
    "print(f\"\\nClassification:\")\n",
    "print(f\"  Total UL importance: {ul_importance_clf['importance'].sum():.4f}\")\n",
    "print(f\"  Mean UL importance: {ul_importance_clf['importance'].mean():.4f}\")\n",
    "print(f\"  Top UL feature: {ul_importance_clf.iloc[0]['feature']} ({ul_importance_clf.iloc[0]['importance']:.4f})\")\n",
    "\n",
    "print(f\"\\nRegression:\")\n",
    "print(f\"  Total UL importance: {ul_importance_reg['importance'].sum():.4f}\")\n",
    "print(f\"  Mean UL importance: {ul_importance_reg['importance'].mean():.4f}\")\n",
    "print(f\"  Top UL feature: {ul_importance_reg.iloc[0]['feature']} ({ul_importance_reg.iloc[0]['importance']:.4f})\")\n",
    "\n",
    "print(f\"\\nTop 10 UL Features (Classification):\")\n",
    "print(ul_importance_clf.head(10)[['feature', 'importance']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nTop 10 UL Features (Regression):\")\n",
    "print(ul_importance_reg.head(10)[['feature', 'importance']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ablation Study - Different UL Feature Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different feature combinations\n",
    "print(\"Testing different UL feature combinations...\\n\")\n",
    "\n",
    "feature_combos = {\n",
    "    'Baseline Only': baseline_features,\n",
    "    '+ Top 5 UL': baseline_features + top_ul_features[:5],\n",
    "    '+ Top 10 UL': baseline_features + top_ul_features[:10],\n",
    "    '+ Top 20 UL': baseline_features + top_ul_features[:20],\n",
    "    '+ All Topics': baseline_features + ul_topic_features + ul_nmf_features,\n",
    "    '+ All PCA': baseline_features + ul_pca_features,\n",
    "    '+ All UL': baseline_features + all_ul_features,\n",
    "}\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "for combo_name, features in feature_combos.items():\n",
    "    print(f\"Testing: {combo_name} ({len(features)} features)...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = prepare_features(df_train, features)\n",
    "    X_test = prepare_features(df_test, features)\n",
    "    \n",
    "    # Train classification model\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "    clf.fit(X_train, y_train_class)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Train regression model\n",
    "    reg = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    reg.fit(X_train, y_train_reg)\n",
    "    y_pred_reg = reg.predict(X_test)\n",
    "    \n",
    "    # Store results\n",
    "    ablation_results.append({\n",
    "        'Combination': combo_name,\n",
    "        'Num Features': len(features),\n",
    "        'F1': f1_score(y_test_class, y_pred),\n",
    "        'Precision': precision_score(y_test_class, y_pred),\n",
    "        'Recall': recall_score(y_test_class, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test_class, y_pred_proba),\n",
    "        'MAE': mean_absolute_error(y_test_reg, y_pred_reg),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test_reg, y_pred_reg)),\n",
    "        'R²': r2_score(y_test_reg, y_pred_reg)\n",
    "    })\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ABLATION STUDY RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(ablation_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ablation study\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# F1 Score\n",
    "ablation_df.plot(x='Combination', y='F1', kind='bar', ax=axes[0, 0], legend=False, color='steelblue')\n",
    "axes[0, 0].set_ylabel('F1 Score')\n",
    "axes[0, 0].set_title('F1 Score by Feature Combination')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ROC-AUC\n",
    "ablation_df.plot(x='Combination', y='ROC-AUC', kind='bar', ax=axes[0, 1], legend=False, color='darkgreen')\n",
    "axes[0, 1].set_ylabel('ROC-AUC')\n",
    "axes[0, 1].set_title('ROC-AUC by Feature Combination')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "ablation_df.plot(x='Combination', y='MAE', kind='bar', ax=axes[1, 0], legend=False, color='coral')\n",
    "axes[1, 0].set_ylabel('MAE (Lower is better)')\n",
    "axes[1, 0].set_title('MAE by Feature Combination')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# R²\n",
    "ablation_df.plot(x='Combination', y='R²', kind='bar', ax=axes[1, 1], legend=False, color='purple')\n",
    "axes[1, 1].set_ylabel('R²')\n",
    "axes[1, 1].set_title('R² by Feature Combination')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*100)\nprint(\"FINAL SUMMARY & RECOMMENDATIONS\")\nprint(\"=\"*100)\n\n# Best performing combination\nbest_f1_idx = ablation_df['F1'].idxmax()\nbest_auc_idx = ablation_df['ROC-AUC'].idxmax()\nbest_r2_idx = ablation_df['R²'].idxmax()\n\nprint(\"\\n1. BEST PERFORMING COMBINATIONS:\")\nprint(f\"   - Best F1 Score: {ablation_df.loc[best_f1_idx, 'Combination']} (F1={ablation_df.loc[best_f1_idx, 'F1']:.4f})\")\nprint(f\"   - Best ROC-AUC: {ablation_df.loc[best_auc_idx, 'Combination']} (AUC={ablation_df.loc[best_auc_idx, 'ROC-AUC']:.4f})\")\nprint(f\"   - Best R²: {ablation_df.loc[best_r2_idx, 'Combination']} (R²={ablation_df.loc[best_r2_idx, 'R²']:.4f})\")\n\n# Calculate improvements\nbaseline_row = ablation_df[ablation_df['Combination'] == 'Baseline Only'].iloc[0]\nbest_row = ablation_df.loc[best_f1_idx]\n\nf1_improvement = ((best_row['F1'] - baseline_row['F1']) / baseline_row['F1'] * 100) if baseline_row['F1'] > 0 else 0\nauc_improvement = ((best_row['ROC-AUC'] - baseline_row['ROC-AUC']) / baseline_row['ROC-AUC'] * 100) if baseline_row['ROC-AUC'] > 0 else 0\nr2_improvement_abs = best_row['R²'] - baseline_row['R²']\n\nprint(\"\\n2. PERFORMANCE IMPROVEMENTS:\")\nprint(f\"   - F1 Score: {f1_improvement:+.2f}%\")\nprint(f\"   - ROC-AUC: {auc_improvement:+.2f}%\")\nprint(f\"   - R² (absolute): {r2_improvement_abs:+.4f}\")\n\n# Interpret results\nprint(\"\\n3. INTERPRETATION:\")\nif baseline_row['R²'] < 0:\n    print(f\"   ⚠️  WARNING: Baseline R² is negative ({baseline_row['R²']:.4f})\")\n    print(f\"   → This means the regression model performs worse than predicting the mean\")\n    print(f\"   → Consider: different features, feature engineering, or different model\")\n\nif baseline_row['F1'] < 0.5:\n    print(f\"   ⚠️  WARNING: F1 score is low ({baseline_row['F1']:.4f})\")\n    print(f\"   → Citation prediction is inherently difficult\")\n    print(f\"   → Consider: focusing on ranking (ROC-AUC) instead of classification\")\n\nif f1_improvement < 5 and auc_improvement < 5:\n    print(f\"   ℹ️  UL features provide modest improvement (<5%)\")\n    print(f\"   → They add some signal but aren't game-changing\")\n    print(f\"   → Consider cost/benefit of feature complexity\")\n\n# Top contributing UL features\ntop_ul_clf = ul_importance_clf.head(5) if len(ul_importance_clf) > 0 else None\nif top_ul_clf is not None and len(top_ul_clf) > 0:\n    print(\"\\n4. TOP 5 UL FEATURES (Classification):\")\n    for i, row in top_ul_clf.iterrows():\n        print(f\"   - {row['feature']}: {row['importance']:.4f}\")\n\nprint(\"\\n5. RECOMMENDATIONS:\")\nprint(f\"   → Focus on improving baseline features first\")\nprint(f\"   → UL features add marginal value - use selectively\")\nprint(f\"   → Consider different problem framing:\")\nprint(f\"      • Ranking papers instead of binary classification\")\nprint(f\"      • Predicting citation percentile instead of raw count\")\nprint(f\"      • Multi-class classification (low/medium/high impact)\")\nprint(f\"\\n6. NEXT STEPS:\")\nprint(f\"   → Investigate why R² is negative (feature engineering issue?)\")\nprint(f\"   → Try gradient boosting models (XGBoost, LightGBM)\")\nprint(f\"   → Add domain-specific features (author h-index, journal impact factor)\")\nprint(f\"   → Consider temporal dynamics (citation accumulation over time)\")\n\nprint(\"\\n\" + \"=\"*100)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_df.to_csv('../data/processed/ul_integration_comparison.csv', index=False)\n",
    "ablation_df.to_csv('../data/processed/ul_ablation_study.csv', index=False)\n",
    "feature_importance_clf.to_csv('../data/processed/ul_feature_importance_clf.csv', index=False)\n",
    "feature_importance_reg.to_csv('../data/processed/ul_feature_importance_reg.csv', index=False)\n",
    "\n",
    "# Save recommended feature list\n",
    "recommended_features = best_row['Combination']\n",
    "with open('../data/processed/recommended_ul_features.txt', 'w') as f:\n",
    "    f.write(f\"Best performing combination: {recommended_features}\\n\")\n",
    "    f.write(f\"Number of features: {best_row['Num Features']}\\n\\n\")\n",
    "    f.write(\"Top 20 UL features by importance:\\n\")\n",
    "    for feat in ul_importance_clf.head(20)['feature']:\n",
    "        f.write(f\"  - {feat}\\n\")\n",
    "\n",
    "print(\"Results saved to data/processed/:\")\n",
    "print(\"  - ul_integration_comparison.csv\")\n",
    "print(\"  - ul_ablation_study.csv\")\n",
    "print(\"  - ul_feature_importance_clf.csv\")\n",
    "print(\"  - ul_feature_importance_reg.csv\")\n",
    "print(\"  - recommended_ul_features.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}