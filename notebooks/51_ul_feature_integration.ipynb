{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Feature Integration\n",
    "\n",
    "**Goal**: Integrate unsupervised learning features into supervised models and measure performance improvement\n",
    "\n",
    "**Approach**:\n",
    "1. Load data with UL features\n",
    "2. Select top-performing UL features based on correlation\n",
    "3. Train baseline models (without UL features)\n",
    "4. Train enhanced models (with UL features)\n",
    "5. Compare performance and analyze improvements\n",
    "6. Feature importance analysis\n",
    "7. Recommendations for production models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with unsupervised features\n",
    "df = pd.read_pickle('../data/processed/data_with_unsupervised_features.pkl')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"\\nDate range: {df['Year'].min()} - {df['Year'].max()}\")\n",
    "print(f\"Citation stats: mean={df['Citations'].mean():.1f}, median={df['Citations'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature categories\n",
    "ul_topic_features = [c for c in df.columns if c.startswith('topic_')]\n",
    "ul_nmf_features = [c for c in df.columns if c.startswith('nmf_topic_')]\n",
    "ul_pca_features = [c for c in df.columns if c.startswith('pca_')]\n",
    "ul_cluster_features = [c for c in df.columns if 'cluster' in c and c != 'venue_cluster']\n",
    "ul_other_features = ['dominant_topic', 'dominant_topic_weight', 'nmf_dominant_topic', \n",
    "                     'citation_zscore', 'is_citation_outlier']\n",
    "\n",
    "all_ul_features = (ul_topic_features + ul_nmf_features + ul_pca_features + \n",
    "                   ul_cluster_features + [f for f in ul_other_features if f in df.columns])\n",
    "\n",
    "print(f\"Unsupervised Learning Features:\")\n",
    "print(f\"  LDA topics: {len(ul_topic_features)}\")\n",
    "print(f\"  NMF topics: {len(ul_nmf_features)}\")\n",
    "print(f\"  PCA components: {len(ul_pca_features)}\")\n",
    "print(f\"  Cluster labels: {len(ul_cluster_features)}\")\n",
    "print(f\"  Other: {len([f for f in ul_other_features if f in df.columns])}\")\n",
    "print(f\"  Total UL features: {len(all_ul_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection - Top UL Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with Citations\n",
    "ul_correlations = df[all_ul_features + ['Citations']].corr()['Citations'].drop('Citations')\n",
    "ul_correlations_abs = ul_correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 30 UL Features by Correlation with Citations:\")\n",
    "print(ul_correlations_abs.head(30))\n",
    "\n",
    "# Select top features\n",
    "top_n = 20\n",
    "top_ul_features = ul_correlations_abs.head(top_n).index.tolist()\n",
    "\n",
    "print(f\"\\nSelected top {top_n} UL features for modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "ul_correlations_abs.head(30).plot(kind='barh')\n",
    "plt.xlabel('Absolute Correlation with Citations')\n",
    "plt.title('Top 30 Unsupervised Features by Correlation')\n",
    "plt.axvline(x=0.05, color='r', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Baseline Features\n",
    "\n",
    "Identify existing engineered features (non-UL) for baseline comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify baseline features (excluding UL features and target)\n",
    "exclude_cols = (\n",
    "    all_ul_features + \n",
    "    ['Citations', 'high_impact', 'log_citations', 'citation_bin', 'citation_category'] +  # targets\n",
    "    ['Title', 'Abstract', 'Authors', 'Author full names', 'Author(s) ID', 'Source title'] +  # text/metadata\n",
    "    ['EID', 'DOI', 'PubMed ID', 'Link', 'Cited by', 'Abstract', 'Document Type']  # identifiers\n",
    ")\n",
    "\n",
    "# Get numeric columns for modeling\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "baseline_features = [c for c in numeric_cols if c not in exclude_cols]\n",
    "\n",
    "print(f\"Available baseline features: {len(baseline_features)}\")\n",
    "print(f\"\\nBaseline features:\")\n",
    "for i, feat in enumerate(baseline_features, 1):\n",
    "    print(f\"  {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification target (high impact = top 25% citations)\n",
    "citation_threshold = df['Citations'].quantile(0.75)\n",
    "df['high_impact'] = (df['Citations'] >= citation_threshold).astype(int)\n",
    "\n",
    "# Create log-transformed regression target\n",
    "df['log_citations'] = np.log1p(df['Citations'])\n",
    "\n",
    "print(f\"Classification target: high_impact (>= {citation_threshold:.0f} citations)\")\n",
    "print(f\"  High impact: {df['high_impact'].sum()} papers ({df['high_impact'].mean()*100:.1f}%)\")\n",
    "print(f\"  Regular: {(~df['high_impact'].astype(bool)).sum()} papers ({(1-df['high_impact'].mean())*100:.1f}%)\")\n",
    "print(f\"\\nRegression target: log(Citations + 1)\")\n",
    "print(f\"  Range: {df['log_citations'].min():.2f} - {df['log_citations'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split - Temporal Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use temporal split: train on earlier years, test on recent years\n",
    "split_year = 2022\n",
    "\n",
    "train_mask = df['Year'] < split_year\n",
    "test_mask = df['Year'] >= split_year\n",
    "\n",
    "df_train = df[train_mask].copy()\n",
    "df_test = df[test_mask].copy()\n",
    "\n",
    "print(f\"Temporal Split (cutoff year: {split_year}):\")\n",
    "print(f\"  Training set: {len(df_train)} papers ({df_train['Year'].min()}-{df_train['Year'].max()})\")\n",
    "print(f\"  Test set: {len(df_test)} papers ({df_test['Year'].min()}-{df_test['Year'].max()})\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Train high_impact: {df_train['high_impact'].mean()*100:.1f}%\")\n",
    "print(f\"  Test high_impact: {df_test['high_impact'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Models (No UL Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data - handle missing values\n",
    "def prepare_features(df, features):\n",
    "    \"\"\"Prepare feature matrix, handling missing values\"\"\"\n",
    "    X = df[features].copy()\n",
    "    # Fill NaN with median for numeric columns\n",
    "    for col in X.columns:\n",
    "        if X[col].isna().any():\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "    return X\n",
    "\n",
    "# Baseline features\n",
    "X_train_baseline = prepare_features(df_train, baseline_features)\n",
    "X_test_baseline = prepare_features(df_test, baseline_features)\n",
    "\n",
    "y_train_class = df_train['high_impact']\n",
    "y_test_class = df_test['high_impact']\n",
    "\n",
    "y_train_reg = df_train['log_citations']\n",
    "y_test_reg = df_test['log_citations']\n",
    "\n",
    "print(f\"Baseline feature matrix: {X_train_baseline.shape}\")\n",
    "print(f\"Features: {baseline_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline classification model\n",
    "print(\"Training Baseline Classification Model (Random Forest)...\")\n",
    "\n",
    "baseline_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "baseline_clf.fit(X_train_baseline, y_train_class)\n",
    "\n",
    "# Predictions\n",
    "y_pred_baseline = baseline_clf.predict(X_test_baseline)\n",
    "y_pred_proba_baseline = baseline_clf.predict_proba(X_test_baseline)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "baseline_f1 = f1_score(y_test_class, y_pred_baseline)\n",
    "baseline_precision = precision_score(y_test_class, y_pred_baseline)\n",
    "baseline_recall = recall_score(y_test_class, y_pred_baseline)\n",
    "baseline_auc = roc_auc_score(y_test_class, y_pred_proba_baseline)\n",
    "\n",
    "print(\"\\nBaseline Classification Results:\")\n",
    "print(f\"  F1 Score: {baseline_f1:.4f}\")\n",
    "print(f\"  Precision: {baseline_precision:.4f}\")\n",
    "print(f\"  Recall: {baseline_recall:.4f}\")\n",
    "print(f\"  ROC-AUC: {baseline_auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline regression model\n",
    "print(\"Training Baseline Regression Model (Random Forest)...\")\n",
    "\n",
    "baseline_reg = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "baseline_reg.fit(X_train_baseline, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_reg_baseline = baseline_reg.predict(X_test_baseline)\n",
    "\n",
    "# Metrics\n",
    "baseline_mae = mean_absolute_error(y_test_reg, y_pred_reg_baseline)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg_baseline))\n",
    "baseline_r2 = r2_score(y_test_reg, y_pred_reg_baseline)\n",
    "\n",
    "print(\"\\nBaseline Regression Results:\")\n",
    "print(f\"  MAE: {baseline_mae:.4f}\")\n",
    "print(f\"  RMSE: {baseline_rmse:.4f}\")\n",
    "print(f\"  R²: {baseline_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Enhanced Models (With Top UL Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced features = baseline + top UL features\n",
    "enhanced_features = baseline_features + top_ul_features\n",
    "\n",
    "X_train_enhanced = prepare_features(df_train, enhanced_features)\n",
    "X_test_enhanced = prepare_features(df_test, enhanced_features)\n",
    "\n",
    "print(f\"Enhanced feature matrix: {X_train_enhanced.shape}\")\n",
    "print(f\"Added {len(top_ul_features)} UL features to {len(baseline_features)} baseline features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train enhanced classification model\n",
    "print(\"Training Enhanced Classification Model (Random Forest + UL)...\")\n",
    "\n",
    "enhanced_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "enhanced_clf.fit(X_train_enhanced, y_train_class)\n",
    "\n",
    "# Predictions\n",
    "y_pred_enhanced = enhanced_clf.predict(X_test_enhanced)\n",
    "y_pred_proba_enhanced = enhanced_clf.predict_proba(X_test_enhanced)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "enhanced_f1 = f1_score(y_test_class, y_pred_enhanced)\n",
    "enhanced_precision = precision_score(y_test_class, y_pred_enhanced)\n",
    "enhanced_recall = recall_score(y_test_class, y_pred_enhanced)\n",
    "enhanced_auc = roc_auc_score(y_test_class, y_pred_proba_enhanced)\n",
    "\n",
    "print(\"\\nEnhanced Classification Results:\")\n",
    "print(f\"  F1 Score: {enhanced_f1:.4f} (Δ={enhanced_f1-baseline_f1:+.4f})\")\n",
    "print(f\"  Precision: {enhanced_precision:.4f} (Δ={enhanced_precision-baseline_precision:+.4f})\")\n",
    "print(f\"  Recall: {enhanced_recall:.4f} (Δ={enhanced_recall-baseline_recall:+.4f})\")\n",
    "print(f\"  ROC-AUC: {enhanced_auc:.4f} (Δ={enhanced_auc-baseline_auc:+.4f})\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_enhanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train enhanced regression model\n",
    "print(\"Training Enhanced Regression Model (Random Forest + UL)...\")\n",
    "\n",
    "enhanced_reg = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "enhanced_reg.fit(X_train_enhanced, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_reg_enhanced = enhanced_reg.predict(X_test_enhanced)\n",
    "\n",
    "# Metrics\n",
    "enhanced_mae = mean_absolute_error(y_test_reg, y_pred_reg_enhanced)\n",
    "enhanced_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg_enhanced))\n",
    "enhanced_r2 = r2_score(y_test_reg, y_pred_reg_enhanced)\n",
    "\n",
    "print(\"\\nEnhanced Regression Results:\")\n",
    "print(f\"  MAE: {enhanced_mae:.4f} (Δ={enhanced_mae-baseline_mae:+.4f})\")\n",
    "print(f\"  RMSE: {enhanced_rmse:.4f} (Δ={enhanced_rmse-baseline_rmse:+.4f})\")\n",
    "print(f\"  R²: {enhanced_r2:.4f} (Δ={enhanced_r2-baseline_r2:+.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall', 'ROC-AUC', 'MAE', 'RMSE', 'R²'],\n",
    "    'Baseline': [baseline_f1, baseline_precision, baseline_recall, baseline_auc, \n",
    "                 baseline_mae, baseline_rmse, baseline_r2],\n",
    "    'Enhanced': [enhanced_f1, enhanced_precision, enhanced_recall, enhanced_auc,\n",
    "                 enhanced_mae, enhanced_rmse, enhanced_r2]\n",
    "})\n",
    "\n",
    "comparison_df['Improvement'] = comparison_df['Enhanced'] - comparison_df['Baseline']\n",
    "comparison_df['Improvement %'] = (comparison_df['Improvement'] / comparison_df['Baseline'] * 100).round(2)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON: Baseline vs Enhanced (with UL features)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Classification metrics\n",
    "class_metrics = ['F1 Score', 'Precision', 'Recall', 'ROC-AUC']\n",
    "class_data = comparison_df[comparison_df['Metric'].isin(class_metrics)]\n",
    "\n",
    "x = np.arange(len(class_metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, class_data['Baseline'], width, label='Baseline', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, class_data['Enhanced'], width, label='Enhanced', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Metric')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('Classification Metrics Comparison')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(class_metrics, rotation=45)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Improvement percentages\n",
    "class_data[['Metric', 'Improvement %']].set_index('Metric')['Improvement %'].plot(\n",
    "    kind='barh', ax=axes[0, 1], color='green', alpha=0.7\n",
    ")\n",
    "axes[0, 1].set_xlabel('Improvement (%)')\n",
    "axes[0, 1].set_title('Classification: % Improvement with UL Features')\n",
    "axes[0, 1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Regression metrics\n",
    "reg_metrics = ['MAE', 'RMSE', 'R²']\n",
    "reg_data = comparison_df[comparison_df['Metric'].isin(reg_metrics)]\n",
    "\n",
    "x = np.arange(len(reg_metrics))\n",
    "\n",
    "axes[1, 0].bar(x - width/2, reg_data['Baseline'], width, label='Baseline', alpha=0.8)\n",
    "axes[1, 0].bar(x + width/2, reg_data['Enhanced'], width, label='Enhanced', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Metric')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_title('Regression Metrics Comparison')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(reg_metrics)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Regression improvement\n",
    "reg_data[['Metric', 'Improvement %']].set_index('Metric')['Improvement %'].plot(\n",
    "    kind='barh', ax=axes[1, 1], color='orange', alpha=0.7\n",
    ")\n",
    "axes[1, 1].set_xlabel('Improvement (%)')\n",
    "axes[1, 1].set_title('Regression: % Improvement with UL Features')\n",
    "axes[1, 1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from enhanced models\n",
    "feature_importance_clf = pd.DataFrame({\n",
    "    'feature': enhanced_features,\n",
    "    'importance': enhanced_clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importance_reg = pd.DataFrame({\n",
    "    'feature': enhanced_features,\n",
    "    'importance': enhanced_reg.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Mark UL features\n",
    "feature_importance_clf['is_ul'] = feature_importance_clf['feature'].isin(top_ul_features)\n",
    "feature_importance_reg['is_ul'] = feature_importance_reg['feature'].isin(top_ul_features)\n",
    "\n",
    "print(\"Top 20 Most Important Features (Classification):\")\n",
    "print(feature_importance_clf.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features (Regression):\")\n",
    "print(feature_importance_reg.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Classification\n",
    "top_20_clf = feature_importance_clf.head(20)\n",
    "colors_clf = ['skyblue' if is_ul else 'lightgray' for is_ul in top_20_clf['is_ul']]\n",
    "\n",
    "axes[0].barh(range(20), top_20_clf['importance'], color=colors_clf)\n",
    "axes[0].set_yticks(range(20))\n",
    "axes[0].set_yticklabels(top_20_clf['feature'])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Top 20 Features: Classification\\n(Blue = UL features)')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Regression\n",
    "top_20_reg = feature_importance_reg.head(20)\n",
    "colors_reg = ['orange' if is_ul else 'lightgray' for is_ul in top_20_reg['is_ul']]\n",
    "\n",
    "axes[1].barh(range(20), top_20_reg['importance'], color=colors_reg)\n",
    "axes[1].set_yticks(range(20))\n",
    "axes[1].set_yticklabels(top_20_reg['feature'])\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Top 20 Features: Regression\\n(Orange = UL features)')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze UL feature contribution\n",
    "ul_importance_clf = feature_importance_clf[feature_importance_clf['is_ul']]\n",
    "ul_importance_reg = feature_importance_reg[feature_importance_reg['is_ul']]\n",
    "\n",
    "print(\"UL Feature Importance Summary:\")\n",
    "print(f\"\\nClassification:\")\n",
    "print(f\"  Total UL importance: {ul_importance_clf['importance'].sum():.4f}\")\n",
    "print(f\"  Mean UL importance: {ul_importance_clf['importance'].mean():.4f}\")\n",
    "print(f\"  Top UL feature: {ul_importance_clf.iloc[0]['feature']} ({ul_importance_clf.iloc[0]['importance']:.4f})\")\n",
    "\n",
    "print(f\"\\nRegression:\")\n",
    "print(f\"  Total UL importance: {ul_importance_reg['importance'].sum():.4f}\")\n",
    "print(f\"  Mean UL importance: {ul_importance_reg['importance'].mean():.4f}\")\n",
    "print(f\"  Top UL feature: {ul_importance_reg.iloc[0]['feature']} ({ul_importance_reg.iloc[0]['importance']:.4f})\")\n",
    "\n",
    "print(f\"\\nTop 10 UL Features (Classification):\")\n",
    "print(ul_importance_clf.head(10)[['feature', 'importance']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nTop 10 UL Features (Regression):\")\n",
    "print(ul_importance_reg.head(10)[['feature', 'importance']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ablation Study - Different UL Feature Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different feature combinations\n",
    "print(\"Testing different UL feature combinations...\\n\")\n",
    "\n",
    "feature_combos = {\n",
    "    'Baseline Only': baseline_features,\n",
    "    '+ Top 5 UL': baseline_features + top_ul_features[:5],\n",
    "    '+ Top 10 UL': baseline_features + top_ul_features[:10],\n",
    "    '+ Top 20 UL': baseline_features + top_ul_features[:20],\n",
    "    '+ All Topics': baseline_features + ul_topic_features + ul_nmf_features,\n",
    "    '+ All PCA': baseline_features + ul_pca_features,\n",
    "    '+ All UL': baseline_features + all_ul_features,\n",
    "}\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "for combo_name, features in feature_combos.items():\n",
    "    print(f\"Testing: {combo_name} ({len(features)} features)...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = prepare_features(df_train, features)\n",
    "    X_test = prepare_features(df_test, features)\n",
    "    \n",
    "    # Train classification model\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "    clf.fit(X_train, y_train_class)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Train regression model\n",
    "    reg = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    reg.fit(X_train, y_train_reg)\n",
    "    y_pred_reg = reg.predict(X_test)\n",
    "    \n",
    "    # Store results\n",
    "    ablation_results.append({\n",
    "        'Combination': combo_name,\n",
    "        'Num Features': len(features),\n",
    "        'F1': f1_score(y_test_class, y_pred),\n",
    "        'Precision': precision_score(y_test_class, y_pred),\n",
    "        'Recall': recall_score(y_test_class, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test_class, y_pred_proba),\n",
    "        'MAE': mean_absolute_error(y_test_reg, y_pred_reg),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test_reg, y_pred_reg)),\n",
    "        'R²': r2_score(y_test_reg, y_pred_reg)\n",
    "    })\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ABLATION STUDY RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(ablation_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ablation study\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# F1 Score\n",
    "ablation_df.plot(x='Combination', y='F1', kind='bar', ax=axes[0, 0], legend=False, color='steelblue')\n",
    "axes[0, 0].set_ylabel('F1 Score')\n",
    "axes[0, 0].set_title('F1 Score by Feature Combination')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ROC-AUC\n",
    "ablation_df.plot(x='Combination', y='ROC-AUC', kind='bar', ax=axes[0, 1], legend=False, color='darkgreen')\n",
    "axes[0, 1].set_ylabel('ROC-AUC')\n",
    "axes[0, 1].set_title('ROC-AUC by Feature Combination')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "ablation_df.plot(x='Combination', y='MAE', kind='bar', ax=axes[1, 0], legend=False, color='coral')\n",
    "axes[1, 0].set_ylabel('MAE (Lower is better)')\n",
    "axes[1, 0].set_title('MAE by Feature Combination')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# R²\n",
    "ablation_df.plot(x='Combination', y='R²', kind='bar', ax=axes[1, 1], legend=False, color='purple')\n",
    "axes[1, 1].set_ylabel('R²')\n",
    "axes[1, 1].set_title('R² by Feature Combination')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"FINAL SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Best performing combination\n",
    "best_f1_idx = ablation_df['F1'].idxmax()\n",
    "best_auc_idx = ablation_df['ROC-AUC'].idxmax()\n",
    "best_r2_idx = ablation_df['R²'].idxmax()\n",
    "\n",
    "print(\"\\n1. BEST PERFORMING COMBINATIONS:\")\n",
    "print(f\"   - Best F1 Score: {ablation_df.loc[best_f1_idx, 'Combination']} (F1={ablation_df.loc[best_f1_idx, 'F1']:.4f})\")\n",
    "print(f\"   - Best ROC-AUC: {ablation_df.loc[best_auc_idx, 'Combination']} (AUC={ablation_df.loc[best_auc_idx, 'ROC-AUC']:.4f})\")\n",
    "print(f\"   - Best R²: {ablation_df.loc[best_r2_idx, 'Combination']} (R²={ablation_df.loc[best_r2_idx, 'R²']:.4f})\")\n",
    "\n",
    "# Calculate improvements\n",
    "baseline_row = ablation_df[ablation_df['Combination'] == 'Baseline Only'].iloc[0]\n",
    "best_row = ablation_df.loc[best_f1_idx]\n",
    "\n",
    "f1_improvement = ((best_row['F1'] - baseline_row['F1']) / baseline_row['F1'] * 100)\n",
    "auc_improvement = ((best_row['ROC-AUC'] - baseline_row['ROC-AUC']) / baseline_row['ROC-AUC'] * 100)\n",
    "r2_improvement = ((best_row['R²'] - baseline_row['R²']) / baseline_row['R²'] * 100)\n",
    "\n",
    "print(\"\\n2. PERFORMANCE IMPROVEMENTS:\")\n",
    "print(f\"   - F1 Score: {f1_improvement:+.2f}%\")\n",
    "print(f\"   - ROC-AUC: {auc_improvement:+.2f}%\")\n",
    "print(f\"   - R²: {r2_improvement:+.2f}%\")\n",
    "\n",
    "# Top contributing UL features\n",
    "top_ul_clf = ul_importance_clf.head(5)\n",
    "print(\"\\n3. TOP 5 UL FEATURES (Classification):\")\n",
    "for i, row in top_ul_clf.iterrows():\n",
    "    print(f\"   - {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDATIONS:\")\n",
    "print(f\"   ✓ Use {best_row['Combination']} for production models\")\n",
    "print(f\"   ✓ Focus on top 10-20 UL features to avoid overfitting\")\n",
    "print(f\"   ✓ NMF topic features (especially nmf_topic_8) are most predictive\")\n",
    "print(f\"   ✓ PCA components add moderate value, consider top 10 only\")\n",
    "print(f\"   ✓ Cluster labels have minimal impact, can be excluded\")\n",
    "print(f\"\\n5. NEXT STEPS:\")\n",
    "print(f\"   → Fine-tune hyperparameters with best feature set\")\n",
    "print(f\"   → Try gradient boosting models (XGBoost, LightGBM)\")\n",
    "print(f\"   → Ensemble baseline + UL models for best performance\")\n",
    "print(f\"   → Deploy model with selected features for production\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_df.to_csv('../data/processed/ul_integration_comparison.csv', index=False)\n",
    "ablation_df.to_csv('../data/processed/ul_ablation_study.csv', index=False)\n",
    "feature_importance_clf.to_csv('../data/processed/ul_feature_importance_clf.csv', index=False)\n",
    "feature_importance_reg.to_csv('../data/processed/ul_feature_importance_reg.csv', index=False)\n",
    "\n",
    "# Save recommended feature list\n",
    "recommended_features = best_row['Combination']\n",
    "with open('../data/processed/recommended_ul_features.txt', 'w') as f:\n",
    "    f.write(f\"Best performing combination: {recommended_features}\\n\")\n",
    "    f.write(f\"Number of features: {best_row['Num Features']}\\n\\n\")\n",
    "    f.write(\"Top 20 UL features by importance:\\n\")\n",
    "    for feat in ul_importance_clf.head(20)['feature']:\n",
    "        f.write(f\"  - {feat}\\n\")\n",
    "\n",
    "print(\"Results saved to data/processed/:\")\n",
    "print(\"  - ul_integration_comparison.csv\")\n",
    "print(\"  - ul_ablation_study.csv\")\n",
    "print(\"  - ul_feature_importance_clf.csv\")\n",
    "print(\"  - ul_feature_importance_reg.csv\")\n",
    "print(\"  - recommended_ul_features.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
