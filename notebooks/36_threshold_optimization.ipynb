{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Classification Threshold for F1 Score\n",
    "\n",
    "Find the optimal probability threshold to maximize F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "X_test = pd.read_pickle('../data/features/X_test_temporal.pkl')\n",
    "y_test = pd.read_pickle('../data/features/y_test_cls_temporal.pkl')\n",
    "\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"High-impact papers: {y_test.sum()} ({y_test.mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or train Logistic Regression (best model)\n",
    "model_path = Path('../models/logistic_regression_classifier.pkl')\n",
    "\n",
    "if model_path.exists():\n",
    "    print(\"Loading saved model...\")\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "else:\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    X_train = pd.read_pickle('../data/features/X_train_temporal.pkl')\n",
    "    y_train = pd.read_pickle('../data/features/y_train_cls_temporal.pkl')\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    Path('../models').mkdir(exist_ok=True)\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "# Get probability predictions\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "print(f\"\\nâœ“ Predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Performance (threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default threshold = 0.5\n",
    "y_pred_default = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "default_f1 = f1_score(y_test, y_pred_default)\n",
    "default_precision = precision_score(y_test, y_pred_default)\n",
    "default_recall = recall_score(y_test, y_pred_default)\n",
    "default_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"CURRENT PERFORMANCE (threshold=0.5):\")\n",
    "print(f\"  ROC-AUC: {default_roc_auc:.4f} ({default_roc_auc*100:.2f}%)\")\n",
    "print(f\"  F1 Score: {default_f1:.4f} ({default_f1*100:.2f}%)\")\n",
    "print(f\"  Precision: {default_precision:.4f} ({default_precision*100:.2f}%)\")\n",
    "print(f\"  Recall: {default_recall:.4f} ({default_recall*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different thresholds\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_f1 = f1_scores[optimal_idx]\n",
    "optimal_precision = precision_scores[optimal_idx]\n",
    "optimal_recall = recall_scores[optimal_idx]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIMAL THRESHOLD FOUND\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOptimal threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"\\nOptimized performance:\")\n",
    "print(f\"  ROC-AUC: {default_roc_auc:.4f} ({default_roc_auc*100:.2f}%) [unchanged]\")\n",
    "print(f\"  F1 Score: {optimal_f1:.4f} ({optimal_f1*100:.2f}%)\")\n",
    "print(f\"  Precision: {optimal_precision:.4f} ({optimal_precision*100:.2f}%)\")\n",
    "print(f\"  Recall: {optimal_recall:.4f} ({optimal_recall*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nImprovement over default (0.5):\")\n",
    "print(f\"  F1: {(optimal_f1 - default_f1)*100:.2f} points ({((optimal_f1/default_f1 - 1)*100):.1f}% increase)\")\n",
    "print(f\"  Precision: {(optimal_precision - default_precision)*100:.2f} points\")\n",
    "print(f\"  Recall: {(optimal_recall - default_recall)*100:.2f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Threshold Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(thresholds, f1_scores, label='F1 Score', linewidth=2)\n",
    "plt.plot(thresholds, precision_scores, label='Precision', linewidth=2, linestyle='--')\n",
    "plt.plot(thresholds, recall_scores, label='Recall', linewidth=2, linestyle='--')\n",
    "\n",
    "# Mark optimal threshold\n",
    "plt.axvline(optimal_threshold, color='red', linestyle=':', linewidth=2, label=f'Optimal ({optimal_threshold:.2f})')\n",
    "plt.axvline(0.5, color='gray', linestyle=':', linewidth=1, label='Default (0.5)', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Classification Threshold', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Classification Metrics vs Threshold', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "figures_dir = Path('../reports/figures')\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(figures_dir / 'threshold_optimization.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions with optimal threshold\n",
    "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Confusion matrices\n",
    "cm_default = confusion_matrix(y_test, y_pred_default)\n",
    "cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Default threshold\n",
    "im1 = axes[0].imshow(cm_default, cmap='Blues')\n",
    "axes[0].set_title(f'Default Threshold (0.5)\\nF1={default_f1:.4f}', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels(['Low', 'High'])\n",
    "axes[0].set_yticklabels(['Low', 'High'])\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0].text(j, i, str(cm_default[i, j]), ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Optimal threshold\n",
    "im2 = axes[1].imshow(cm_optimal, cmap='Blues')\n",
    "axes[1].set_title(f'Optimal Threshold ({optimal_threshold:.2f})\\nF1={optimal_f1:.4f}', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_yticks([0, 1])\n",
    "axes[1].set_xticklabels(['Low', 'High'])\n",
    "axes[1].set_yticklabels(['Low', 'High'])\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j, i, str(cm_optimal[i, j]), ha='center', va='center', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'confusion_matrix_threshold_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimal threshold for deployment\n",
    "threshold_info = {\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "    'f1_score': optimal_f1,\n",
    "    'precision': optimal_precision,\n",
    "    'recall': optimal_recall,\n",
    "    'roc_auc': default_roc_auc\n",
    "}\n",
    "\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(models_dir / 'optimal_threshold.pkl', 'wb') as f:\n",
    "    pickle.dump(threshold_info, f)\n",
    "\n",
    "print(\"âœ“ Optimal threshold saved to models/optimal_threshold.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"THRESHOLD OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nBEFORE (threshold=0.5):\")\n",
    "print(f\"  ROC-AUC: {default_roc_auc*100:.2f}%\")\n",
    "print(f\"  F1 Score: {default_f1*100:.2f}%\")\n",
    "print(f\"  Precision: {default_precision*100:.2f}%\")\n",
    "print(f\"  Recall: {default_recall*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nAFTER (threshold={optimal_threshold:.2f}):\")\n",
    "print(f\"  ROC-AUC: {default_roc_auc*100:.2f}% [unchanged]\")\n",
    "print(f\"  F1 Score: {optimal_f1*100:.2f}% (+{(optimal_f1-default_f1)*100:.2f})\")\n",
    "print(f\"  Precision: {optimal_precision*100:.2f}% (+{(optimal_precision-default_precision)*100:.2f})\")\n",
    "print(f\"  Recall: {optimal_recall*100:.2f}% (+{(optimal_recall-default_recall)*100:.2f})\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Use threshold={optimal_threshold:.2f} for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
