{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Check for Cleaned Dataset\n",
    "\n",
    "Comprehensive check for:\n",
    "1. Missing values\n",
    "2. Duplicates\n",
    "3. Outliers\n",
    "4. Data consistency\n",
    "5. Feature integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_pickle('../data/processed/cleaned_data.pkl')\n",
    "print(f\"Cleaned data: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# Load features\n",
    "X = pd.read_pickle('../data/features/X_all.pkl')\n",
    "y_class = pd.read_pickle('../data/features/y_classification.pkl')\n",
    "y_reg = pd.read_pickle('../data/features/y_regression.pkl')\n",
    "\n",
    "print(f\"Feature matrix: {X.shape}\")\n",
    "print(f\"Classification target: {len(y_class)}\")\n",
    "print(f\"Regression target: {len(y_reg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BASIC DATA INTEGRITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated(subset='EID')\n",
    "print(f\"\\nDuplicate EIDs: {duplicates.sum()}\")\n",
    "if duplicates.sum() > 0:\n",
    "    print(f\"  ⚠️  WARNING: Found {duplicates.sum()} duplicate papers\")\n",
    "    print(\"\\nDuplicate papers:\")\n",
    "    print(df[duplicates][['EID', 'Title', 'Year']])\n",
    "else:\n",
    "    print(f\"  ✓ No duplicates found\")\n",
    "\n",
    "# Check index alignment\n",
    "print(f\"\\nIndex alignment:\")\n",
    "print(f\"  df: {len(df)}\")\n",
    "print(f\"  X: {len(X)}\")\n",
    "print(f\"  y_class: {len(y_class)}\")\n",
    "print(f\"  y_reg: {len(y_reg)}\")\n",
    "\n",
    "if len(df) == len(X) == len(y_class) == len(y_reg):\n",
    "    print(f\"  ✓ All indices aligned\")\n",
    "else:\n",
    "    print(f\"  ⚠️  WARNING: Index mismatch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check missing in key columns\n",
    "key_columns = ['EID', 'Title', 'Year', 'Abstract', 'Citations',\n",
    "               'Number of Authors', 'Number of Institutions', 'Number of Countries/Regions',\n",
    "               'SNIP (publication year)', 'CiteScore (publication year)', 'SJR (publication year)']\n",
    "\n",
    "print(\"\\nMissing values in key columns:\")\n",
    "missing_summary = []\n",
    "for col in key_columns:\n",
    "    if col in df.columns:\n",
    "        missing = df[col].isnull().sum()\n",
    "        missing_pct = (missing / len(df)) * 100\n",
    "        missing_summary.append({\n",
    "            'Column': col,\n",
    "            'Missing': missing,\n",
    "            'Percentage': f\"{missing_pct:.1f}%\"\n",
    "        })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_summary)\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# Feature matrix missing values\n",
    "print(f\"\\nMissing values in feature matrix:\")\n",
    "missing_features = X.isnull().sum()\n",
    "if missing_features.sum() > 0:\n",
    "    print(f\"  ⚠️  WARNING: {missing_features.sum()} missing values\")\n",
    "    print(f\"\\nColumns with missing values:\")\n",
    "    print(missing_features[missing_features > 0])\n",
    "else:\n",
    "    print(f\"  ✓ No missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Citation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CITATION STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nCitation distribution:\")\n",
    "print(df['Citations'].describe())\n",
    "\n",
    "# Check for negative citations\n",
    "negative = (df['Citations'] < 0).sum()\n",
    "if negative > 0:\n",
    "    print(f\"\\n⚠️  WARNING: {negative} papers with negative citations!\")\n",
    "    print(df[df['Citations'] < 0][['EID', 'Title', 'Citations']])\n",
    "else:\n",
    "    print(f\"\\n✓ No negative citations\")\n",
    "\n",
    "# Extreme outliers\n",
    "p999 = df['Citations'].quantile(0.999)\n",
    "extreme = df[df['Citations'] > p999]\n",
    "print(f\"\\nExtreme outliers (>99.9 percentile = {p999:.0f}):\")\n",
    "print(f\"  Count: {len(extreme)}\")\n",
    "if len(extreme) > 0:\n",
    "    print(f\"\\nTop 10 most cited papers:\")\n",
    "    top_cited = df.nlargest(10, 'Citations')[['Title', 'Year', 'Citations']]\n",
    "    print(top_cited.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Year Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"YEAR DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nPapers by year:\")\n",
    "year_counts = df['Year'].value_counts().sort_index()\n",
    "print(year_counts)\n",
    "\n",
    "# Check for unexpected years\n",
    "expected_years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "unexpected = df[~df['Year'].isin(expected_years)]\n",
    "if len(unexpected) > 0:\n",
    "    print(f\"\\n⚠️  WARNING: {len(unexpected)} papers with unexpected years:\")\n",
    "    print(unexpected[['EID', 'Title', 'Year']])\n",
    "else:\n",
    "    print(f\"\\n✓ All years within expected range (2015-2020)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Author Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"AUTHOR FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nNumber of authors:\")\n",
    "print(df['Number of Authors'].describe())\n",
    "\n",
    "# Check for zero authors\n",
    "zero_authors = (df['Number of Authors'] == 0).sum()\n",
    "if zero_authors > 0:\n",
    "    print(f\"\\n⚠️  WARNING: {zero_authors} papers with 0 authors!\")\n",
    "else:\n",
    "    print(f\"\\n✓ No papers with 0 authors\")\n",
    "\n",
    "print(f\"\\nNumber of institutions:\")\n",
    "print(df['Number of Institutions'].describe())\n",
    "\n",
    "print(f\"\\nNumber of countries:\")\n",
    "print(df['Number of Countries/Regions'].describe())\n",
    "\n",
    "# Check logical consistency\n",
    "inconsistent = df[df['Number of Institutions'] > df['Number of Authors']]\n",
    "if len(inconsistent) > 0:\n",
    "    print(f\"\\n⚠️  WARNING: {len(inconsistent)} papers with more institutions than authors!\")\n",
    "    print(inconsistent[['Title', 'Number of Authors', 'Number of Institutions']].head())\n",
    "else:\n",
    "    print(f\"\\n✓ No papers with more institutions than authors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Venue Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VENUE FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "venue_cols = ['SNIP (publication year)', 'CiteScore (publication year)', 'SJR (publication year)']\n",
    "\n",
    "for col in venue_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Non-null: {df[col].notna().sum()} ({df[col].notna().sum()/len(df)*100:.1f}%)\")\n",
    "        if df[col].notna().sum() > 0:\n",
    "            print(df[col].describe())\n",
    "            \n",
    "            # Check for negative\n",
    "            negative = (df[col] < 0).sum()\n",
    "            if negative > 0:\n",
    "                print(f\"  ⚠️  WARNING: {negative} negative values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Features (Abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEXT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Abstract lengths\n",
    "df['abstract_length'] = df['Abstract'].fillna('').str.len()\n",
    "print(f\"\\nAbstract lengths:\")\n",
    "print(df['abstract_length'].describe())\n",
    "\n",
    "# Empty abstracts\n",
    "empty = (df['abstract_length'] == 0).sum()\n",
    "if empty > 0:\n",
    "    print(f\"\\n⚠️  WARNING: {empty} papers with empty abstracts!\")\n",
    "    print(df[df['abstract_length'] == 0][['EID', 'Title', 'Year']])\n",
    "else:\n",
    "    print(f\"\\n✓ No empty abstracts\")\n",
    "\n",
    "# Very short abstracts\n",
    "short = (df['abstract_length'] < 50).sum()\n",
    "if short > 0:\n",
    "    print(f\"\\nℹ️  {short} papers with very short abstracts (<50 chars):\")\n",
    "    print(df[df['abstract_length'] < 50][['Title', 'abstract_length', 'Abstract']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Matrix Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE MATRIX VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Infinite values\n",
    "inf_values = np.isinf(X).sum().sum()\n",
    "if inf_values > 0:\n",
    "    print(f\"\\n⚠️  WARNING: {inf_values} infinite values!\")\n",
    "    inf_cols = X.columns[np.isinf(X).any()]\n",
    "    print(f\"Columns with inf values: {list(inf_cols)}\")\n",
    "else:\n",
    "    print(f\"\\n✓ No infinite values\")\n",
    "\n",
    "# NaN values\n",
    "nan_values = X.isnull().sum().sum()\n",
    "if nan_values > 0:\n",
    "    print(f\"\\n⚠️  WARNING: {nan_values} NaN values!\")\n",
    "else:\n",
    "    print(f\"✓ No NaN values\")\n",
    "\n",
    "# Constant features\n",
    "constant_features = X.columns[X.std() == 0]\n",
    "if len(constant_features) > 0:\n",
    "    print(f\"\\n⚠️  WARNING: {len(constant_features)} constant features (zero variance):\")\n",
    "    print(list(constant_features[:20]))\n",
    "    if len(constant_features) > 20:\n",
    "        print(f\"... and {len(constant_features) - 20} more\")\n",
    "else:\n",
    "    print(f\"✓ No constant features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Target Variable Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TARGET VARIABLE VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Classification target\n",
    "print(f\"\\nClassification target:\")\n",
    "print(f\"  High-impact (1): {y_class.sum()} ({y_class.mean()*100:.1f}%)\")\n",
    "print(f\"  Low-impact (0): {(~y_class).sum()} ({(1-y_class.mean())*100:.1f}%)\")\n",
    "\n",
    "threshold = df['Citations'].quantile(0.75)\n",
    "print(f\"  Threshold: {threshold:.0f} citations (75th percentile)\")\n",
    "\n",
    "# Check alignment\n",
    "expected_high = (df['Citations'] >= threshold).sum()\n",
    "if y_class.sum() != expected_high:\n",
    "    print(f\"\\n⚠️  WARNING: Target mismatch! Expected {expected_high}, got {y_class.sum()}\")\n",
    "else:\n",
    "    print(f\"\\n✓ Target aligned with 75th percentile\")\n",
    "\n",
    "# Regression target\n",
    "print(f\"\\nRegression target:\")\n",
    "print(y_reg.describe())\n",
    "\n",
    "# Check if matches citations\n",
    "if not y_reg.equals(df['Citations']):\n",
    "    print(f\"\\n⚠️  WARNING: Regression target doesn't match Citations!\")\n",
    "else:\n",
    "    print(f\"\\n✓ Regression target matches Citations column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Temporal Split Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEMPORAL SPLIT VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train = pd.read_pickle('../data/features/X_train_temporal.pkl')\n",
    "X_test = pd.read_pickle('../data/features/X_test_temporal.pkl')\n",
    "\n",
    "print(f\"\\nTemporal split:\")\n",
    "print(f\"  Train (2015-2017): {len(X_train)}\")\n",
    "print(f\"  Test (2018-2020): {len(X_test)}\")\n",
    "print(f\"  Total: {len(X_train) + len(X_test)}\")\n",
    "\n",
    "# Check sum\n",
    "if len(X_train) + len(X_test) == len(X):\n",
    "    print(f\"\\n✓ Train + Test = Total\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  WARNING: Train + Test ≠ Total ({len(X_train)} + {len(X_test)} ≠ {len(X)})\")\n",
    "\n",
    "# Check overlap\n",
    "train_indices = set(X_train.index)\n",
    "test_indices = set(X_test.index)\n",
    "overlap = train_indices & test_indices\n",
    "if len(overlap) > 0:\n",
    "    print(f\"\\n⚠️  WARNING: {len(overlap)} papers in both train and test!\")\n",
    "else:\n",
    "    print(f\"✓ No overlap between train and test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY CHECK COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nReview the output above for any ⚠️  WARNING messages.\")\n",
    "print(\"All ✓ checks indicate the data is clean and ready for modeling.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
