{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "Analyze prediction errors to understand model weaknesses:\n",
    "1. Load models and predictions\n",
    "2. Identify misclassified papers\n",
    "3. Analyze false positives and false negatives\n",
    "4. Compare characteristics of correct vs incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_pickle('../data/features/X_test_temporal.pkl')\n",
    "y_test_cls = pd.read_pickle('../data/features/y_test_cls_temporal.pkl')\n",
    "y_test_reg = pd.read_pickle('../data/features/y_test_reg_temporal.pkl')\n",
    "metadata_test = pd.read_pickle('../data/features/metadata_test.pkl')\n",
    "\n",
    "with open('../models/classification/lightgbm.pkl', 'rb') as f:\n",
    "    clf_model = pickle.load(f)\n",
    "\n",
    "with open('../models/regression/random_forest.pkl', 'rb') as f:\n",
    "    reg_model = pickle.load(f)\n",
    "\n",
    "y_reg_raw = pd.read_pickle('../data/features/y_regression.pkl')\n",
    "y_test_reg_raw = y_reg_raw[metadata_test.index]\n",
    "\n",
    "print(f\"Test set: {len(X_test)} papers\")\n",
    "print(f\"High-impact: {y_test_cls.sum()} ({y_test_cls.mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cls = clf_model.predict(X_test)\n",
    "y_pred_proba = clf_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_reg = reg_model.predict(X_test)\n",
    "\n",
    "results_df = metadata_test.copy()\n",
    "results_df['actual_citations'] = y_test_reg_raw.values\n",
    "results_df['actual_log_citations'] = y_test_reg.values\n",
    "results_df['predicted_log_citations'] = y_pred_reg\n",
    "results_df['predicted_citations'] = np.expm1(y_pred_reg)\n",
    "results_df['actual_high_impact'] = y_test_cls.values\n",
    "results_df['predicted_high_impact'] = y_pred_cls\n",
    "results_df['high_impact_probability'] = y_pred_proba\n",
    "results_df['prediction_error'] = np.abs(y_test_reg.values - y_pred_reg)\n",
    "\n",
    "print(\"Results dataframe created\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['classification_status'] = 'Correct'\n",
    "results_df.loc[(results_df['actual_high_impact'] == 1) & (results_df['predicted_high_impact'] == 0), 'classification_status'] = 'False Negative'\n",
    "results_df.loc[(results_df['actual_high_impact'] == 0) & (results_df['predicted_high_impact'] == 1), 'classification_status'] = 'False Positive'\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "print(results_df['classification_status'].value_counts())\n",
    "print(f\"\\nAccuracy: {(results_df['classification_status'] == 'Correct').mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = results_df[results_df['classification_status'] == 'False Positive'].sort_values('high_impact_probability', ascending=False)\n",
    "\n",
    "print(f\"False Positives: {len(false_positives)}\")\n",
    "print(f\"\\nTop 10 False Positives (model was most confident):\")\n",
    "print(false_positives[['Title', 'Year', 'actual_citations', 'predicted_citations', 'high_impact_probability']].head(10))\n",
    "\n",
    "print(f\"\\nFalse Positive Statistics:\")\n",
    "print(f\"Mean actual citations: {false_positives['actual_citations'].mean():.1f}\")\n",
    "print(f\"Mean predicted citations: {false_positives['predicted_citations'].mean():.1f}\")\n",
    "print(f\"Mean confidence: {false_positives['high_impact_probability'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = results_df[results_df['classification_status'] == 'False Negative'].sort_values('high_impact_probability', ascending=True)\n",
    "\n",
    "print(f\"False Negatives: {len(false_negatives)}\")\n",
    "if len(false_negatives) > 0:\n",
    "    print(f\"\\nTop 10 False Negatives (model was least confident):\")\n",
    "    print(false_negatives[['Title', 'Year', 'actual_citations', 'predicted_citations', 'high_impact_probability']].head(10))\n",
    "    \n",
    "    print(f\"\\nFalse Negative Statistics:\")\n",
    "    print(f\"Mean actual citations: {false_negatives['actual_citations'].mean():.1f}\")\n",
    "    print(f\"Mean predicted citations: {false_negatives['predicted_citations'].mean():.1f}\")\n",
    "    print(f\"Mean confidence: {false_negatives['high_impact_probability'].mean():.3f}\")\n",
    "else:\n",
    "    print(\"No false negatives! Model catches all high-impact papers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_errors = results_df.nlargest(20, 'prediction_error')\n",
    "\n",
    "print(\"Top 20 Largest Prediction Errors:\")\n",
    "print(top_errors[['Title', 'Year', 'actual_citations', 'predicted_citations', 'prediction_error']])\n",
    "\n",
    "print(f\"\\nMean absolute error (log scale): {results_df['prediction_error'].mean():.4f}\")\n",
    "print(f\"Median absolute error (log scale): {results_df['prediction_error'].median():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Distribution by Citation Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['citation_bin'] = pd.cut(results_df['actual_citations'], bins=[0, 5, 10, 25, 50, 100, 1000, 100000], labels=['0-5', '6-10', '11-25', '26-50', '51-100', '101-1000', '1000+'])\n",
    "\n",
    "error_by_bin = results_df.groupby('citation_bin')['prediction_error'].agg(['mean', 'median', 'count'])\n",
    "print(\"Prediction Error by Citation Range:\")\n",
    "print(error_by_bin)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "results_df.boxplot(column='prediction_error', by='citation_bin', ax=ax)\n",
    "ax.set_xlabel('Citation Range')\n",
    "ax.set_ylabel('Prediction Error (log scale)')\n",
    "ax.set_title('Prediction Error Distribution by Citation Range')\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Overestimation vs Underestimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['signed_error'] = y_pred_reg - y_test_reg.values\n",
    "results_df['error_type'] = 'Accurate'\n",
    "results_df.loc[results_df['signed_error'] > 0.5, 'error_type'] = 'Overestimated'\n",
    "results_df.loc[results_df['signed_error'] < -0.5, 'error_type'] = 'Underestimated'\n",
    "\n",
    "print(\"Prediction Bias:\")\n",
    "print(results_df['error_type'].value_counts())\n",
    "print(f\"\\nMean signed error: {results_df['signed_error'].mean():.4f}\")\n",
    "print(f\"(Positive = overestimation, Negative = underestimation)\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(results_df['signed_error'], bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Perfect prediction')\n",
    "ax.set_xlabel('Signed Prediction Error (log scale)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Signed Prediction Errors')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ERROR ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nClassification Errors:\")\n",
    "print(f\"  False Positives: {len(false_positives)} (predicted high-impact, but actually low)\")\n",
    "print(f\"  False Negatives: {len(false_negatives)} (predicted low-impact, but actually high)\")\n",
    "print(f\"  Accuracy: {(results_df['classification_status'] == 'Correct').mean()*100:.2f}%\")\n",
    "print(f\"\\nRegression Errors:\")\n",
    "print(f\"  Mean absolute error: {results_df['prediction_error'].mean():.4f} (log scale)\")\n",
    "print(f\"  Mean signed error: {results_df['signed_error'].mean():.4f}\")\n",
    "if results_df['signed_error'].mean() > 0:\n",
    "    print(f\"  Model tends to OVERESTIMATE citation counts\")\n",
    "else:\n",
    "    print(f\"  Model tends to UNDERESTIMATE citation counts\")\n",
    "print(f\"\\nKey Insight: Where does the model struggle most?\")\n",
    "print(error_by_bin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
