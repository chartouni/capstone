{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "Analyze prediction errors to understand model weaknesses:\n",
    "1. Load models and predictions\n",
    "2. Identify misclassified papers\n",
    "3. Analyze false positives and false negatives\n",
    "4. Compare characteristics of correct vs incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../')\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', None)\n%matplotlib inline\n\n# Create figures directory\nfigures_dir = Path('../reports/figures')\nfigures_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"Figures will be saved to: {figures_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_pickle('../data/features/X_test_temporal.pkl')\n",
    "y_test_cls = pd.read_pickle('../data/features/y_test_cls_temporal.pkl')\n",
    "y_test_reg = pd.read_pickle('../data/features/y_test_reg_temporal.pkl')\n",
    "metadata_test = pd.read_pickle('../data/features/metadata_test.pkl')\n",
    "\n",
    "with open('../models/classification/lightgbm.pkl', 'rb') as f:\n",
    "    clf_model = pickle.load(f)\n",
    "\n",
    "with open('../models/regression/random_forest.pkl', 'rb') as f:\n",
    "    reg_model = pickle.load(f)\n",
    "\n",
    "y_reg_raw = pd.read_pickle('../data/features/y_regression.pkl')\n",
    "y_test_reg_raw = y_reg_raw[metadata_test.index]\n",
    "\n",
    "print(f\"Test set: {len(X_test)} papers\")\n",
    "print(f\"High-impact: {y_test_cls.sum()} ({y_test_cls.mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cls = clf_model.predict(X_test)\n",
    "y_pred_proba = clf_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_reg = reg_model.predict(X_test)\n",
    "\n",
    "results_df = metadata_test.copy()\n",
    "results_df['actual_citations'] = y_test_reg_raw.values\n",
    "results_df['actual_log_citations'] = y_test_reg.values\n",
    "results_df['predicted_log_citations'] = y_pred_reg\n",
    "results_df['predicted_citations'] = np.expm1(y_pred_reg)\n",
    "results_df['actual_high_impact'] = y_test_cls.values\n",
    "results_df['predicted_high_impact'] = y_pred_cls\n",
    "results_df['high_impact_probability'] = y_pred_proba\n",
    "results_df['prediction_error'] = np.abs(y_test_reg.values - y_pred_reg)\n",
    "\n",
    "print(\"Results dataframe created\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['classification_status'] = 'Correct'\n",
    "results_df.loc[(results_df['actual_high_impact'] == 1) & (results_df['predicted_high_impact'] == 0), 'classification_status'] = 'False Negative'\n",
    "results_df.loc[(results_df['actual_high_impact'] == 0) & (results_df['predicted_high_impact'] == 1), 'classification_status'] = 'False Positive'\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "print(results_df['classification_status'].value_counts())\n",
    "print(f\"\\nAccuracy: {(results_df['classification_status'] == 'Correct').mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = results_df[results_df['classification_status'] == 'False Positive'].sort_values('high_impact_probability', ascending=False)\n",
    "\n",
    "print(f\"False Positives: {len(false_positives)}\")\n",
    "print(f\"\\nTop 10 False Positives (model was most confident):\")\n",
    "print(false_positives[['Title', 'Year', 'actual_citations', 'predicted_citations', 'high_impact_probability']].head(10))\n",
    "\n",
    "print(f\"\\nFalse Positive Statistics:\")\n",
    "print(f\"Mean actual citations: {false_positives['actual_citations'].mean():.1f}\")\n",
    "print(f\"Mean predicted citations: {false_positives['predicted_citations'].mean():.1f}\")\n",
    "print(f\"Mean confidence: {false_positives['high_impact_probability'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = results_df[results_df['classification_status'] == 'False Negative'].sort_values('high_impact_probability', ascending=True)\n",
    "\n",
    "print(f\"False Negatives: {len(false_negatives)}\")\n",
    "if len(false_negatives) > 0:\n",
    "    print(f\"\\nTop 10 False Negatives (model was least confident):\")\n",
    "    print(false_negatives[['Title', 'Year', 'actual_citations', 'predicted_citations', 'high_impact_probability']].head(10))\n",
    "    \n",
    "    print(f\"\\nFalse Negative Statistics:\")\n",
    "    print(f\"Mean actual citations: {false_negatives['actual_citations'].mean():.1f}\")\n",
    "    print(f\"Mean predicted citations: {false_negatives['predicted_citations'].mean():.1f}\")\n",
    "    print(f\"Mean confidence: {false_negatives['high_impact_probability'].mean():.3f}\")\n",
    "else:\n",
    "    print(\"No false negatives! Model catches all high-impact papers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_errors = results_df.nlargest(20, 'prediction_error')\n",
    "\n",
    "print(\"Top 20 Largest Prediction Errors:\")\n",
    "print(top_errors[['Title', 'Year', 'actual_citations', 'predicted_citations', 'prediction_error']])\n",
    "\n",
    "print(f\"\\nMean absolute error (log scale): {results_df['prediction_error'].mean():.4f}\")\n",
    "print(f\"Median absolute error (log scale): {results_df['prediction_error'].median():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Distribution by Citation Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results_df['citation_bin'] = pd.cut(results_df['actual_citations'], bins=[0, 5, 10, 25, 50, 100, 1000, 100000], labels=['0-5', '6-10', '11-25', '26-50', '51-100', '101-1000', '1000+'])\n\nerror_by_bin = results_df.groupby('citation_bin')['prediction_error'].agg(['mean', 'median', 'count'])\nprint(\"Prediction Error by Citation Range:\")\nprint(error_by_bin)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nresults_df.boxplot(column='prediction_error', by='citation_bin', ax=ax)\nax.set_xlabel('Citation Range')\nax.set_ylabel('Prediction Error (log scale)')\nax.set_title('Prediction Error Distribution by Citation Range')\nplt.sca(ax)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig(figures_dir / 'error_by_citation_range.png', dpi=300, bbox_inches='tight')\nprint(f\"Saved: {figures_dir / 'error_by_citation_range.png'}\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Overestimation vs Underestimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results_df['signed_error'] = y_pred_reg - y_test_reg.values\nresults_df['error_type'] = 'Accurate'\nresults_df.loc[results_df['signed_error'] > 0.5, 'error_type'] = 'Overestimated'\nresults_df.loc[results_df['signed_error'] < -0.5, 'error_type'] = 'Underestimated'\n\nprint(\"Prediction Bias:\")\nprint(results_df['error_type'].value_counts())\nprint(f\"\\nMean signed error: {results_df['signed_error'].mean():.4f}\")\nprint(f\"(Positive = overestimation, Negative = underestimation)\")\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.hist(results_df['signed_error'], bins=50, edgecolor='black', alpha=0.7)\nax.axvline(0, color='red', linestyle='--', linewidth=2, label='Perfect prediction')\nax.set_xlabel('Signed Prediction Error (log scale)')\nax.set_ylabel('Count')\nax.set_title('Distribution of Signed Prediction Errors')\nax.legend()\nplt.tight_layout()\nplt.savefig(figures_dir / 'signed_error_distribution.png', dpi=300, bbox_inches='tight')\nprint(f\"Saved: {figures_dir / 'signed_error_distribution.png'}\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ERROR ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nClassification Errors:\")\n",
    "print(f\"  False Positives: {len(false_positives)} (predicted high-impact, but actually low)\")\n",
    "print(f\"  False Negatives: {len(false_negatives)} (predicted low-impact, but actually high)\")\n",
    "print(f\"  Accuracy: {(results_df['classification_status'] == 'Correct').mean()*100:.2f}%\")\n",
    "print(f\"\\nRegression Errors:\")\n",
    "print(f\"  Mean absolute error: {results_df['prediction_error'].mean():.4f} (log scale)\")\n",
    "print(f\"  Mean signed error: {results_df['signed_error'].mean():.4f}\")\n",
    "if results_df['signed_error'].mean() > 0:\n",
    "    print(f\"  Model tends to OVERESTIMATE citation counts\")\n",
    "else:\n",
    "    print(f\"  Model tends to UNDERESTIMATE citation counts\")\n",
    "print(f\"\\nKey Insight: Where does the model struggle most?\")\n",
    "print(error_by_bin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}