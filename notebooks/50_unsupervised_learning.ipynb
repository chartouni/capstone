{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning on AUB Dataset\n",
    "\n",
    "**Goal**: Apply unsupervised learning techniques to discover patterns, clusters, and latent features in the AUB publication dataset\n",
    "\n",
    "**Techniques**:\n",
    "1. Topic Modeling (LDA) - Discover research themes in abstracts\n",
    "2. Clustering - Group similar papers, authors, venues\n",
    "3. Dimensionality Reduction - PCA, t-SNE, UMAP for visualization\n",
    "4. Anomaly Detection - Identify outliers and unusual papers\n",
    "5. Feature Discovery - Extract new features for downstream models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA, TruncatedSVD, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned AUB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_pickle('../data/processed/cleaned_data.pkl')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Modeling with LDA\n",
    "\n",
    "Discover latent research topics in paper abstracts using Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text data - using Abstract column\n",
    "abstracts = df['Abstract'].fillna('').astype(str)\n",
    "\n",
    "print(f\"Total abstracts: {len(abstracts)}\")\n",
    "print(f\"Non-empty abstracts: {(abstracts.str.len() > 0).sum()}\")\n",
    "print(f\"\\nSample abstract:\\n{abstracts.iloc[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize abstracts with CountVectorizer for LDA\n",
    "print(\"Vectorizing abstracts...\")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=2000,\n",
    "    stop_words='english',\n",
    "    min_df=5,  # Minimum document frequency\n",
    "    max_df=0.7,  # Maximum document frequency\n",
    "    ngram_range=(1, 2)  # Unigrams and bigrams\n",
    ")\n",
    "\n",
    "X_counts = vectorizer.fit_transform(abstracts)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"Document-term matrix shape: {X_counts.shape}\")\n",
    "print(f\"Sparsity: {(1 - X_counts.nnz / (X_counts.shape[0] * X_counts.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model with multiple topic counts to find optimal\n",
    "print(\"Training LDA models with different topic counts...\\n\")\n",
    "\n",
    "topic_counts = [5, 10, 15, 20, 25]\n",
    "perplexities = []\n",
    "log_likelihoods = []\n",
    "\n",
    "for n_topics in topic_counts:\n",
    "    print(f\"Training LDA with {n_topics} topics...\")\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_iter=20,\n",
    "        learning_method='batch'\n",
    "    )\n",
    "    lda.fit(X_counts)\n",
    "    \n",
    "    perplexity = lda.perplexity(X_counts)\n",
    "    log_likelihood = lda.score(X_counts)\n",
    "    \n",
    "    perplexities.append(perplexity)\n",
    "    log_likelihoods.append(log_likelihood)\n",
    "    \n",
    "    print(f\"  Perplexity: {perplexity:.2f}\")\n",
    "    print(f\"  Log-likelihood: {log_likelihood:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perplexity vs number of topics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(topic_counts, perplexities, marker='o', linewidth=2)\n",
    "axes[0].set_xlabel('Number of Topics')\n",
    "axes[0].set_ylabel('Perplexity')\n",
    "axes[0].set_title('LDA Perplexity vs Number of Topics\\n(Lower is better)')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(topic_counts, log_likelihoods, marker='o', linewidth=2, color='orange')\n",
    "axes[1].set_xlabel('Number of Topics')\n",
    "axes[1].set_ylabel('Log-Likelihood')\n",
    "axes[1].set_title('LDA Log-Likelihood vs Number of Topics\\n(Higher is better)')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final LDA model with optimal topic count\n",
    "optimal_topics = 15  # Adjust based on the plot above\n",
    "\n",
    "print(f\"Training final LDA model with {optimal_topics} topics...\")\n",
    "\n",
    "lda_final = LatentDirichletAllocation(\n",
    "    n_components=optimal_topics,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    max_iter=30,\n",
    "    learning_method='batch',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "topic_distributions = lda_final.fit_transform(X_counts)\n",
    "\n",
    "print(f\"\\nTopic distribution matrix shape: {topic_distributions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "n_top_words = 15\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOP WORDS PER TOPIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "topic_keywords = {}\n",
    "for topic_idx, topic in enumerate(lda_final.components_):\n",
    "    top_indices = topic.argsort()[-n_top_words:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_indices]\n",
    "    topic_keywords[topic_idx] = top_words\n",
    "    \n",
    "    print(f\"\\nTopic {topic_idx}:\")\n",
    "    print(f\"  {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add topic features to dataframe\n",
    "topic_cols = [f'topic_{i}' for i in range(optimal_topics)]\n",
    "df_topics = pd.DataFrame(topic_distributions, columns=topic_cols, index=df.index)\n",
    "\n",
    "# Dominant topic for each paper\n",
    "df['dominant_topic'] = topic_distributions.argmax(axis=1)\n",
    "df['dominant_topic_weight'] = topic_distributions.max(axis=1)\n",
    "\n",
    "# Add all topic distributions\n",
    "df = pd.concat([df, df_topics], axis=1)\n",
    "\n",
    "print(\"Topic features added to dataframe!\")\n",
    "print(f\"New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze topics\n",
    "print(\"Topic Distribution:\")\n",
    "print(df['dominant_topic'].value_counts().sort_index())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Papers per topic\n",
    "df['dominant_topic'].value_counts().sort_index().plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_xlabel('Topic ID')\n",
    "axes[0].set_ylabel('Number of Papers')\n",
    "axes[0].set_title('Distribution of Papers Across Topics')\n",
    "axes[0].grid(axis='y')\n",
    "\n",
    "# Average citations by topic\n",
    "df.groupby('dominant_topic')['Citations'].mean().plot(kind='bar', ax=axes[1], color='orange')\n",
    "axes[1].set_xlabel('Topic ID')\n",
    "axes[1].set_ylabel('Average Citations')\n",
    "axes[1].set_title('Average Citations by Dominant Topic')\n",
    "axes[1].grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF and NMF Topic Modeling\n",
    "\n",
    "Alternative topic modeling using Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization\n",
    "print(\"Creating TF-IDF representation...\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,\n",
    "    stop_words='english',\n",
    "    min_df=5,\n",
    "    max_df=0.7,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(abstracts)\n",
    "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NMF model\n",
    "print(\"Training NMF topic model...\")\n",
    "\n",
    "nmf = NMF(\n",
    "    n_components=optimal_topics,\n",
    "    random_state=42,\n",
    "    max_iter=200,\n",
    "    init='nndsvda'\n",
    ")\n",
    "\n",
    "nmf_topics = nmf.fit_transform(X_tfidf)\n",
    "print(f\"NMF topic matrix shape: {nmf_topics.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display NMF topics\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NMF TOPICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    top_indices = topic.argsort()[-n_top_words:][::-1]\n",
    "    top_words = [tfidf_feature_names[i] for i in top_indices]\n",
    "    \n",
    "    print(f\"\\nNMF Topic {topic_idx}:\")\n",
    "    print(f\"  {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add NMF topics to dataframe\n",
    "nmf_cols = [f'nmf_topic_{i}' for i in range(optimal_topics)]\n",
    "df_nmf = pd.DataFrame(nmf_topics, columns=nmf_cols, index=df.index)\n",
    "\n",
    "df['nmf_dominant_topic'] = nmf_topics.argmax(axis=1)\n",
    "df = pd.concat([df, df_nmf], axis=1)\n",
    "\n",
    "print(\"NMF features added!\")\n",
    "print(f\"New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction & Visualization\n",
    "\n",
    "Use PCA and t-SNE to visualize the high-dimensional abstract space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on TF-IDF features\n",
    "print(\"Running PCA...\")\n",
    "\n",
    "n_pca_components = 50\n",
    "pca = PCA(n_components=n_pca_components, random_state=42)\n",
    "X_pca = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "print(f\"PCA shape: {X_pca.shape}\")\n",
    "print(f\"Explained variance (first 10 components): {pca.explained_variance_ratio_[:10]}\")\n",
    "print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual variance\n",
    "axes[0].bar(range(1, 21), pca.explained_variance_ratio_[:20])\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('PCA Explained Variance (First 20 Components)')\n",
    "axes[0].grid(axis='y')\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, n_pca_components + 1), \n",
    "             np.cumsum(pca.explained_variance_ratio_), \n",
    "             marker='o', linewidth=2)\n",
    "axes[1].axhline(y=0.8, color='r', linestyle='--', label='80% variance')\n",
    "axes[1].axhline(y=0.9, color='orange', linestyle='--', label='90% variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PCA features to dataframe\n",
    "pca_cols = [f'pca_{i}' for i in range(n_pca_components)]\n",
    "df_pca = pd.DataFrame(X_pca, columns=pca_cols, index=df.index)\n",
    "df = pd.concat([df, df_pca], axis=1)\n",
    "\n",
    "print(\"PCA features added!\")\n",
    "print(f\"New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# t-SNE for 2D visualization (sample subset for speed)\nprint(\"Running t-SNE (this may take a few minutes)...\")\n\n# Use a sample for t-SNE if dataset is large\nsample_size = min(5000, len(df))\nsample_idx = np.random.choice(len(df), sample_size, replace=False)\n\ntsne = TSNE(\n    n_components=2,\n    random_state=42,\n    perplexity=30,\n    max_iter=1000,\n    verbose=1\n)\n\nX_tsne = tsne.fit_transform(X_pca[sample_idx])\nprint(f\"t-SNE shape: {X_tsne.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t-SNE colored by dominant topic\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Color by topic\n",
    "scatter1 = axes[0].scatter(\n",
    "    X_tsne[:, 0], \n",
    "    X_tsne[:, 1],\n",
    "    c=df.iloc[sample_idx]['dominant_topic'],\n",
    "    cmap='tab20',\n",
    "    alpha=0.6,\n",
    "    s=10\n",
    ")\n",
    "axes[0].set_title('t-SNE Visualization Colored by Dominant Topic')\n",
    "axes[0].set_xlabel('t-SNE 1')\n",
    "axes[0].set_ylabel('t-SNE 2')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Topic ID')\n",
    "\n",
    "# Color by citations (log scale)\n",
    "scatter2 = axes[1].scatter(\n",
    "    X_tsne[:, 0],\n",
    "    X_tsne[:, 1],\n",
    "    c=np.log1p(df.iloc[sample_idx]['Citations']),\n",
    "    cmap='viridis',\n",
    "    alpha=0.6,\n",
    "    s=10\n",
    ")\n",
    "axes[1].set_title('t-SNE Visualization Colored by Citations (log scale)')\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Log(Citations + 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering Analysis\n",
    "\n",
    "Apply K-Means clustering to discover paper groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of clusters using elbow method and silhouette score\n",
    "print(\"Finding optimal number of clusters...\\n\")\n",
    "\n",
    "k_range = range(3, 16)\n",
    "inertias = []\n",
    "silhouette_scores_list = []\n",
    "ch_scores = []\n",
    "\n",
    "# Use PCA features for clustering\n",
    "X_cluster = X_pca[:, :20]  # Use first 20 PCA components\n",
    "\n",
    "for k in k_range:\n",
    "    print(f\"Testing k={k}...\")\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_cluster)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores_list.append(silhouette_score(X_cluster, labels))\n",
    "    ch_scores.append(calinski_harabasz_score(X_cluster, labels))\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clustering metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(k_range, inertias, marker='o', linewidth=2)\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Silhouette score\n",
    "axes[1].plot(k_range, silhouette_scores_list, marker='o', linewidth=2, color='orange')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score (Higher is better)')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Calinski-Harabasz score\n",
    "axes[2].plot(k_range, ch_scores, marker='o', linewidth=2, color='green')\n",
    "axes[2].set_xlabel('Number of Clusters (k)')\n",
    "axes[2].set_ylabel('Calinski-Harabasz Score')\n",
    "axes[2].set_title('Calinski-Harabasz Score (Higher is better)')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best k based on silhouette score\n",
    "best_k_idx = np.argmax(silhouette_scores_list)\n",
    "best_k = list(k_range)[best_k_idx]\n",
    "print(f\"\\nOptimal k based on silhouette score: {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final K-Means model\n",
    "optimal_k = best_k  # Use the optimal k from above\n",
    "\n",
    "print(f\"Training K-Means with k={optimal_k}...\")\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)\n",
    "df['kmeans_cluster'] = kmeans_final.fit_predict(X_cluster)\n",
    "\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(df['kmeans_cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters\n",
    "cluster_analysis = df.groupby('kmeans_cluster').agg({\n",
    "    'Citations': ['mean', 'median', 'std', 'count'],\n",
    "    'Year': ['min', 'max', 'mean'],\n",
    "}).round(2)\n",
    "\n",
    "print(\"Cluster Characteristics:\")\n",
    "print(cluster_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Cluster sizes\n",
    "df['kmeans_cluster'].value_counts().sort_index().plot(kind='bar', ax=axes[0, 0])\n",
    "axes[0, 0].set_xlabel('Cluster ID')\n",
    "axes[0, 0].set_ylabel('Number of Papers')\n",
    "axes[0, 0].set_title('Papers per Cluster')\n",
    "axes[0, 0].grid(axis='y')\n",
    "\n",
    "# Average citations by cluster\n",
    "df.groupby('kmeans_cluster')['Citations'].mean().plot(kind='bar', ax=axes[0, 1], color='orange')\n",
    "axes[0, 1].set_xlabel('Cluster ID')\n",
    "axes[0, 1].set_ylabel('Average Citations')\n",
    "axes[0, 1].set_title('Average Citations by Cluster')\n",
    "axes[0, 1].grid(axis='y')\n",
    "\n",
    "# Cluster visualization on t-SNE (if available)\n",
    "if 'X_tsne' in locals():\n",
    "    scatter = axes[1, 0].scatter(\n",
    "        X_tsne[:, 0],\n",
    "        X_tsne[:, 1],\n",
    "        c=df.iloc[sample_idx]['kmeans_cluster'],\n",
    "        cmap='tab10',\n",
    "        alpha=0.6,\n",
    "        s=10\n",
    "    )\n",
    "    axes[1, 0].set_title('K-Means Clusters on t-SNE Space')\n",
    "    axes[1, 0].set_xlabel('t-SNE 1')\n",
    "    axes[1, 0].set_ylabel('t-SNE 2')\n",
    "    plt.colorbar(scatter, ax=axes[1, 0], label='Cluster ID')\n",
    "\n",
    "# Year distribution by cluster\n",
    "df.boxplot(column='Year', by='kmeans_cluster', ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('Cluster ID')\n",
    "axes[1, 1].set_ylabel('Publication Year')\n",
    "axes[1, 1].set_title('Year Distribution by Cluster')\n",
    "plt.suptitle('')  # Remove default boxplot title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection\n",
    "\n",
    "Identify outlier papers using DBSCAN and statistical methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN for outlier detection\n",
    "print(\"Running DBSCAN for anomaly detection...\")\n",
    "\n",
    "dbscan = DBSCAN(eps=3, min_samples=10)\n",
    "df['dbscan_cluster'] = dbscan.fit_predict(X_cluster)\n",
    "\n",
    "# -1 indicates outliers\n",
    "n_outliers = (df['dbscan_cluster'] == -1).sum()\n",
    "n_clusters_dbscan = len(set(df['dbscan_cluster'])) - (1 if -1 in df['dbscan_cluster'].values else 0)\n",
    "\n",
    "print(f\"\\nDBSCAN found:\")\n",
    "print(f\"  Clusters: {n_clusters_dbscan}\")\n",
    "print(f\"  Outliers: {n_outliers} ({n_outliers/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze outliers\n",
    "outliers = df[df['dbscan_cluster'] == -1]\n",
    "\n",
    "print(\"\\nOutlier Statistics:\")\n",
    "print(f\"Mean citations: {outliers['Citations'].mean():.2f} vs {df['Citations'].mean():.2f} (overall)\")\n",
    "print(f\"Median citations: {outliers['Citations'].median():.0f} vs {df['Citations'].median():.0f} (overall)\")\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    print(\"\\nTop 10 outliers by citations:\")\n",
    "    print(outliers.nlargest(10, 'Citations')[['Year', 'Citations', 'dominant_topic']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical outlier detection based on citations\n",
    "from scipy import stats\n",
    "\n",
    "# Z-score method\n",
    "df['citation_zscore'] = np.abs(stats.zscore(df['Citations']))\n",
    "df['is_citation_outlier'] = df['citation_zscore'] > 3\n",
    "\n",
    "print(f\"\\nCitation outliers (|z| > 3): {df['is_citation_outlier'].sum()}\")\n",
    "print(\"\\nTop citation outliers:\")\n",
    "print(df[df['is_citation_outlier']].nlargest(10, 'Citations')[['Year', 'Citations', 'citation_zscore']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Correlation Analysis\n",
    "\n",
    "Analyze relationships between unsupervised features and citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of unsupervised features with citations\n",
    "unsupervised_features = (\n",
    "    topic_cols + \n",
    "    nmf_cols + \n",
    "    pca_cols[:10] +  # First 10 PCA components\n",
    "    ['dominant_topic', 'nmf_dominant_topic', 'kmeans_cluster']\n",
    ")\n",
    "\n",
    "# Correlations\n",
    "correlations = df[unsupervised_features + ['Citations']].corr()['Citations'].drop('Citations')\n",
    "correlations = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 correlations with Citations:\")\n",
    "print(correlations.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlations.head(20).plot(kind='barh')\n",
    "plt.xlabel('Absolute Correlation with Citations')\n",
    "plt.title('Top 20 Unsupervised Features by Correlation with Citations')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of features added\n",
    "original_cols = pd.read_pickle('../data/processed/cleaned_data.pkl').columns\n",
    "new_cols = [col for col in df.columns if col not in original_cols]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"UNSUPERVISED LEARNING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOriginal dataset: {len(original_cols)} columns\")\n",
    "print(f\"New dataset: {len(df.columns)} columns\")\n",
    "print(f\"Features added: {len(new_cols)}\")\n",
    "print(f\"\\nNew feature categories:\")\n",
    "print(f\"  - LDA topics: {len(topic_cols)}\")\n",
    "print(f\"  - NMF topics: {len(nmf_cols)}\")\n",
    "print(f\"  - PCA components: {len(pca_cols)}\")\n",
    "print(f\"  - Cluster labels: {len([c for c in new_cols if 'cluster' in c])}\")\n",
    "print(f\"  - Other: {len(new_cols) - len(topic_cols) - len(nmf_cols) - len(pca_cols) - len([c for c in new_cols if 'cluster' in c])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced dataset with unsupervised features\n",
    "output_path = Path('../data/processed/data_with_unsupervised_features.pkl')\n",
    "df.to_pickle(output_path)\n",
    "\n",
    "print(f\"\\nEnhanced dataset saved to: {output_path}\")\n",
    "print(f\"File size: {output_path.stat().st_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature list for documentation\n",
    "feature_summary = pd.DataFrame({\n",
    "    'feature': new_cols,\n",
    "    'type': ['topic' if 'topic' in c else 'nmf' if 'nmf' in c else 'pca' if 'pca' in c else 'cluster' if 'cluster' in c else 'other' for c in new_cols]\n",
    "})\n",
    "\n",
    "feature_summary.to_csv('../data/processed/unsupervised_features_list.csv', index=False)\n",
    "print(\"\\nFeature list saved to: ../data/processed/unsupervised_features_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook applied multiple unsupervised learning techniques to the AUB dataset:\n",
    "\n",
    "1. ✅ **Topic Modeling** - LDA and NMF discovered research themes in abstracts\n",
    "2. ✅ **Dimensionality Reduction** - PCA, t-SNE for feature extraction and visualization\n",
    "3. ✅ **Clustering** - K-Means and DBSCAN grouped similar papers\n",
    "4. ✅ **Anomaly Detection** - Identified outlier papers\n",
    "5. ✅ **Feature Engineering** - Created new features for downstream models\n",
    "\n",
    "**Next steps:**\n",
    "- Use these unsupervised features in your supervised models\n",
    "- Compare model performance with/without unsupervised features\n",
    "- Analyze which unsupervised features are most predictive\n",
    "- Consider additional techniques (autoencoders, word embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}